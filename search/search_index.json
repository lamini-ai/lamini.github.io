{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lamini \ud83e\udd99","text":"<p>Build mini-agents with 90%+ accuracy, whether you're a solo developer or an enterprise team. Get started with $300 in free credits.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Goal Description Link \ud83d\ude80 Get Started Boost your mini LLMs from 50% to 90%+ accuracy Quick Start \ud83d\udca1 Try It Out Test your PDF knowledge base Playground (with Memory RAG) \ud83c\udfaf Memory Tuning Build accurate, efficient models Memory Tuning \ud83e\udd16 RAG Tools Create reliable mini-agents Memory RAG \ud83c\udfaf Classification Deploy scalable classifiers Classifier Agent \ud83d\udd12 Self-Hosted Install Lamini on your own GPUs Kubernetes Install <p>Questions? Contact us. We read every message \u2014 or at one of our mini-agents does :)</p>"},{"location":"#core-products","title":"Core Products","text":""},{"location":"#memory-tuning-paper-class-with-andrew-ng-meta-about","title":"Memory Tuning [paper] [class with Andrew Ng &amp; Meta] [about]","text":"<p>Build the most accurate and efficient fine-tuned models:</p> <ul> <li> <p>Inject precise facts to eliminate hallucinations</p> </li> <li> <p>Start with only 10 facts &amp; examples, scale to 100,000+</p> </li> <li> <p>Reliably get 95%+ accuracy (removes accuracy ceilings on many tasks)</p> </li> <li> <p>Keep latency and costs low, by getting away with memory-tuned smaller LMs and mini-agents</p> </li> <li> <p>One API, any open model</p> </li> </ul>"},{"location":"#memory-rag-paper-about","title":"Memory RAG [paper] [about]","text":"<p>Skip the complex RAG setup. Easier than Memory Tuning. Get 90%+ accuracy out of the box:</p> <ul> <li> <p>Boost accuracy from 50% to 90-95% compared to GPT4, after just a few iterations on your data and telling the model how to improve</p> </li> <li> <p>Smart embedding that expands your data representation to capture true meaning and relationships</p> </li> <li> <p>Build reliable, specializedmini-agents that work together</p> </li> <li> <p>Simple API, powerful results</p> </li> </ul>"},{"location":"#classifier-agent-toolkit-demo-about","title":"Classifier Agent Toolkit [demo] [about]","text":"<p>Build accurate classifiers in minutes, not months:</p> <ul> <li> <p>Handle any number of categories, from 2 to 1000+</p> </li> <li> <p>Process unstructured data at scale with 400K tokens/second</p> </li> <li> <p>Route requests automatically, with 99.9% accuracy</p> </li> <li> <p>Triage code and content efficiently</p> </li> </ul>"},{"location":"#perfect-for","title":"Perfect For","text":""},{"location":"#developers-startups","title":"Developers &amp; Startups","text":"<ul> <li>Simple SDK and API</li> <li>Start free, scale as you grow</li> <li>Clear documentation and examples</li> <li>Fast integration into your stack, OpenAI API compatible</li> </ul>"},{"location":"#enterprise-teams","title":"Enterprise Teams","text":"<ul> <li>Production-ready security</li> <li>Air-gapped deployment option</li> <li>Scale across departments</li> <li>Custom deployment support</li> <li>Reduce production risks with 99.9% accuracy</li> </ul>"},{"location":"#real-world-applications","title":"Real-World Applications","text":"<p>Build what matters to you:</p> <ul> <li> <p>SQL Generator: Convert natural language to database queries</p> </li> <li> <p>Customer Support Agent: Scale customer service intelligently</p> </li> <li> <p>Data Classifier: Automate manual sorting and labeling</p> </li> <li> <p>Code Helper: Build assistants for any programming language</p> </li> <li> <p>Mini-Agent: Automate planning and execution of specialized tasks</p> </li> </ul>"},{"location":"#getting-started-is-easy","title":"Getting Started Is Easy","text":"<ol> <li>Start with $300 in free credits</li> <li>Choose your deployment (cloud or self-hosted)</li> <li>Use our SDKs or API</li> <li>Monitor through our dashboard</li> </ol> <pre><code># It's really this simple\nfrom lamini import Lamini\n\nmodel = Lamini(open_source_model_on_huggingface)\nmodel.tune(your_data)\nresponse = model.generate(your_prompt)\n</code></pre>"},{"location":"#who-are-we","title":"Who are we?","text":"<p>Lamini's team has been training, fine-tuning, and preference-tuning LLMs over the past two decades. We invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI\u2019s GPT-3 and GPT-4, Anthropic\u2019s Claude, Meta\u2019s Llama 3.1, Google\u2019s PaLM, and NVIDIA\u2019s Megatron.</p>"},{"location":"#whats-new","title":"What's new?","text":"<p>Check out our blog for the latest updates.  </p> <p></p>"},{"location":"about/","title":"About","text":""},{"location":"about/#what-is-lamini","title":"What is Lamini?","text":"<p>Lamini provides the best LLM inference and tuning for the enterprise. Factual LLMs. Up in 10min. Deployed anywhere.</p>"},{"location":"about/#lamini-platform","title":"Lamini Platform","text":"<p>Lamini Platform orchestrates GPUs to deliver exceptional LLM tuning and inference capabilities, which easily integrate into enterprise applications via the Lamini Python client,REST API, and web UI.</p> <p></p> <p>See for yourself: take a quick tour (with free API access!) to see how Lamini works, or contact us to run in your own environment.</p>"},{"location":"about/#deployment-models","title":"Deployment Models","text":"<p>Lamini Platform is available in three different deployment models:</p> <ul> <li>On-Demand: fully-managed training and inference at https://app.lamini.ai, with pay-as-you-go pricing.</li> <li>Reserved: dedicated GPUs for your organization, hosted on Lamini's infrastructure, with per-GPU pricing.</li> <li>Self-Managed: run Lamini Platform in your environment on your GPUs (on premise, in your VPC, even air-gapped deployments), with per-GPU pricing.</li> </ul>"},{"location":"about/#whats-unique-about-lamini","title":"What's unique about Lamini?","text":"Area Problem Lamini's solution Tuning Hallucinations 95% accuracy on factual tasks: memory tuning Tuning High infrastructure costs 32x model compression: Memory Tuning with efficient LoRAs Inference Unreliable app integrations 100% accurate JSON schema output: structured output"},{"location":"about/#who-are-we","title":"Who are we?","text":"<p>Lamini's team has been finetuning LLMs over the past two decades: we invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI\u2019s GPT-3 and GPT-4, Anthropic\u2019s Claude, Meta\u2019s Llama 3.1, Google\u2019s PaLM, and NVIDIA\u2019s Megatron.</p>"},{"location":"about/#whats-new","title":"What's new?","text":"<p>Check out our blog for the latest updates.</p> <p></p>"},{"location":"api/","title":"REST API","text":""},{"location":"authenticate/","title":"API Authentication","text":""},{"location":"authenticate/#1-get-your-lamini-api-key","title":"1. Get your Lamini API key \ud83d\udd11","text":"<p>Your API key is at https://app.lamini.ai/account. If it's your first time, create a free account by logging in.</p> <p>If you're self-managing Lamini Platform on your own GPUs, check out the OIDC authentication docs for setting up user auth.</p>"},{"location":"authenticate/#2-authenticate","title":"2. Authenticate","text":"Environment VariableConfig FilePython Client <p>Add your key to your environment variables. In your terminal, run:</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <p>Put this line in your <code>~/.bash_profile</code> or equivalent file, so you don't have to rerun it in a new session. Remember to <code>source ~/.bash_profile</code> after you make the change.</p> <pre><code>echo \"export LAMINI_API_KEY='$LAMINI_API_KEY'\" &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\necho $LAMINI_API_KEY\n</code></pre> <p>You can authenticate by writing the following to a file <code>~/.lamini/configure.yaml</code></p> <pre><code>production:\n    key: &lt;YOUR-LAMINI-API-KEY&gt;\n</code></pre> <p>For convenience, you can also authenticate directly in a python environment after importing lamini. It's recommended to use the other two methods.</p> <pre><code>import lamini\nlamini.api_key = \"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre>"},{"location":"authenticate/#advanced-python-setup-vpc-or-on-premise","title":"Advanced Python setup: VPC or on premise","text":"<p>If you are running Lamini in your VPC or on prem, you can change the URL from Lamini's hosted service to your own server URL:</p> Python scriptIn <code>~/.lamini/configure.yaml</code> <p>Test that it works: <pre><code>llm = Lamini(\n    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    api_key=\"&lt;YOUR-LAMINI-API-KEY&gt;\",\n    api_url=\"&lt;YOUR-SERVER-URL-HERE&gt;\",\n)\nresponse = llm.generate(\"Tell me a story about llamas.\")\n\nprint(response)\n</code></pre></p> <p>Add the extra <code>url</code> field:</p> <pre><code>production:\n    key: \"&lt;YOUR-LAMINI-API-KEY&gt;\"\n    url: \"&lt;YOUR-SERVER-URL-HERE&gt;\"\n</code></pre>"},{"location":"authenticate/#google-colab","title":"Google Colab","text":"<p>Here's a Colab notebook you can use to get started: Getting Started with Lamini</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#core-development-questions","title":"Core Development Questions","text":""},{"location":"faq/#how-do-i-set-up-authentication","title":"How do I set up authentication?","text":"<p>See the Authentication guide for getting and configuring your Lamini API key.</p>"},{"location":"faq/#how-does-model-loading-work","title":"How does model loading work?","text":"<ul> <li>Model weights are loaded to GPU memory once and persist between requests</li> <li>Loading only happens on initial startup or after unexpected events</li> <li>Loading time scales with model size</li> </ul>"},{"location":"faq/#what-systems-can-i-develop-with-lamini-on","title":"What systems can I develop with Lamini on?","text":"<ul> <li>Recommended: Ubuntu 22.04+ with Python 3.10-3.12</li> <li>Not officially supported on Windows (use Docker with Linux container instead)</li> </ul>"},{"location":"faq/#training-tuning","title":"Training &amp; Tuning","text":""},{"location":"faq/#what-models-can-i-use","title":"What models can I use?","text":"<p>Check the Models page for the full list of supported models.</p>"},{"location":"faq/#how-long-can-training-jobs-run","title":"How long can training jobs run?","text":"<ul> <li>Default timeout: 4 hours</li> <li>Jobs automatically checkpoint and resume if timeout occurs</li> <li>For longer runs:</li> <li>Request more GPUs via <code>gpu_config</code></li> <li>Contact us for dedicated instances</li> </ul>"},{"location":"faq/#can-i-disable-memory-tuning-mome","title":"Can I disable memory tuning (MoME)?","text":"<p>Yes, use these settings for cases like summarization where qualitative output is preferred: <pre><code>finetune_args={\n  \"batch_size\": 1,\n  \"index_ivf_nlist\": 1,\n  \"index_method\": \"IndexFlatL2\",\n  \"index_max_size\": 1,\n}\n</code></pre></p>"},{"location":"faq/#how-does-lamini-optimize-model-training","title":"How does Lamini optimize model training?","text":"<ul> <li>Uses LoRAs (low-rank adapters) automatically</li> <li>266x fewer parameters than full model finetuning</li> <li>1.09B times faster model switching</li> <li>No manual configuration needed</li> </ul>"},{"location":"faq/#infrastructure","title":"Infrastructure","text":""},{"location":"faq/#why-might-my-job-be-queued","title":"Why might my job be queued?","text":"<p>The On-Demand plan uses shared resources. For dedicated compute: - Consider Lamini Reserved plans - Contact us about running on your own infrastructure</p>"},{"location":"faq/#what-gpu-can-lamini-run-on","title":"What GPU can Lamini run on?","text":"<ul> <li>Lamini can run on AMD and NVIDIA GPUs</li> </ul>"},{"location":"faq/#how-do-i-get-started-with-lamini-private-servers-or-enterprise-plans","title":"How do I get started with Lamini private servers or enterprise plans?","text":"<ul> <li>Contact us to learn more about our reserved plans</li> <li>Run your own jobs on dedicated compute</li> </ul>"},{"location":"models/","title":"Supported Models","text":""},{"location":"models/#supported-models","title":"Supported Models","text":""},{"location":"models/#lamini-on-demand","title":"Lamini On-Demand","text":"<p>Lamini On-Demand supports a variety of the most popular open source LLMs, including Llama 3.1, Mistral 3, Phi-3, Qwen 2, and many more.</p> <p>Models available on Lamini On-Demand for inference and tuning:</p> <ul> <li><code>EleutherAI/pythia-410m</code></li> <li><code>EleutherAI/pythia-70m</code></li> <li><code>hf-internal-testing/tiny-random-gpt2</code></li> <li><code>meta-llama/Llama-2-13b-chat-hf</code></li> <li><code>meta-llama/Llama-2-7b-chat-hf</code></li> <li><code>meta-llama/Llama-2-7b-hf</code></li> <li><code>meta-llama/Meta-Llama-3-8B-Instruct</code></li> <li><code>meta-llama/Meta-Llama-3.1-8B-Instruct</code></li> <li><code>microsoft/phi-2</code></li> <li><code>microsoft/Phi-3-mini-4k-instruct</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.1</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.2</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.3</code></li> <li><code>Qwen/Qwen2-7B-Instruct</code></li> </ul>"},{"location":"models/#lamini-reserved-and-self-managed","title":"Lamini Reserved and Self-Managed","text":"<p>Lamini Reserved and Self-Managed support all CausalLM models from Hugging Face (excluding those requiring Flash Attention 2 or 3). Roughly 95% of all models on HF are supported. If you're interested in using models that aren't available in Lamini On-Demand, please contact us.</p>"},{"location":"models/#model-size-and-performance","title":"Model size and performance","text":"<p>With Memory Tuning you can achieve very high factual accuracy with 8B models, without giving up fluent generalization. Using smaller models lowers operating costs and improves latency.</p> <p>Some factors to consider when thinking about model size:</p> <ul> <li>The more active parameters a model has, the more GPU memory is required to use the model.</li> <li>If a model is larger than a single GPU's memory, it needs to run across multiple GPUs. This means exchanging more data across the network, and both inference and tuning will take longer.</li> <li>Tuning requires significantly more GPU memory than inference.</li> </ul>"},{"location":"models/#model-loading","title":"Model loading","text":"<p>Lamini On-Demand only allows use of the models listed above.</p> <p>If you're using Lamini Reserved or Self-Managed, you can configure your cluster to use any supported Hugging Face model.</p> <p>The <code>batch_model_list</code> in <code>llama_config.yaml</code> lets you specify which models to preload onto your allocated inference GPUs. Inference requests for all other models will be handled by your allocated <code>catchall</code> GPUs, and those models will be loaded from Hugging Face when requested. </p> <p>Because models are large (usually tens of GBs), downloading them from Hugging Face and then loading them into GPU memory takes time. Please allow 20-30 minutes for non-preloaded models to load. Requests for models that have not yet loaded will return an error.</p> <p>We recommend focusing development on one model or a small set of models, and preloading them. We've seen the highest accuracy and performance gains come from improving data quality and tuning recipes, rather than testing many models hoping to find one that works significantly better out of the box.</p>"},{"location":"quick_start/","title":"Quick Start","text":"<p>Let's get you started on Lamini, so you can be on your way to boosting your mini LLMs from 50% to 90%+ accuracy \u2014 and building your own mini-agents!</p>"},{"location":"quick_start/#setup","title":"Setup","text":"<p>Install the Python SDK <pre><code>pip install --upgrade lamini\n</code></pre></p> <p>Get your Lamini API key (with $300 in free credits) at https://app.lamini.ai/account.</p> <p>Export your key as an environment variable so it's easy to use later.</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre>"},{"location":"quick_start/#run-inference-to-check-its-running","title":"Run inference to check it's running","text":"<pre><code>from lamini import Lamini\n\nllm = Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\n\nprint(\n    llm.generate(\n        \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers facts the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    )\n)\n\nfrom lamini import Lamini\n</code></pre>"},{"location":"quick_start/#expected-output","title":"Expected Output","text":"<pre><code>While animals are known for their impressive memory abilities, some species stand out for their exceptional memory capabilities. Here are some of the top contenders:\n...\n(a long list of animals and discussion about their merits)\n...\nSo, while there are many animals with impressive memory abilities, the octopus takes the crown for its remarkable cognitive abilities!\n</code></pre> <p>Yikes, that's wordy! Plus, we all know llamas are the smart ones. Let's make it more accurate.</p>"},{"location":"quick_start/#memory-tune-a-model","title":"Memory Tune a model","text":""},{"location":"quick_start/#start-the-tuning-job","title":"Start the tuning job","text":"<p>Note</p> <p>We're skipping important steps here. Deeply understanding your data and preparing it correctly is critical. For the best results, follow the process in Memory Tuning.</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\ndata = [\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal has the best memory?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"A llama!&lt;|eot_id|&gt;\",\n    },\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers things the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"A llama!&lt;|eot_id|&gt;\",\n    },\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;What are some other smart animals?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"Dolphins, ravens, and chimpanzees.&lt;|eot_id|&gt;\",\n    },\n]\n\nllm.tune(data_or_dataset_id=data)\n</code></pre>"},{"location":"quick_start/#check-the-status-of-your-tuning-job","title":"Check the status of your tuning job","text":"<pre><code>from lamini import Lamini\n\nllm = Lamini(\"\")\n\nprint(llm.get_jobs()[0])\n</code></pre>"},{"location":"quick_start/#try-the-tuned-model","title":"Try the tuned model","text":"<p>After the status is \"COMPLETED\", you can run inference with the tuned model to see how it performs.</p> <p>Info</p> <p>Substitute your own tuned <code>model_name</code> in the code below.</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(\"model_name_of_your_tuned_model\")\n\nprint(\n    llm.generate(\n        \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers facts the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    )\n)\n\nfrom lamini import Lamini\n</code></pre>"},{"location":"quick_start/#expected-output_1","title":"Expected Output","text":"<pre><code>A llama!\n</code></pre> <p>That's much better!</p> <p>Learn more about tuning</p>"},{"location":"quick_start/#next-steps","title":"Next steps","text":"<ul> <li>Try out the Playground with Memory RAG</li> <li>Build a Classifier Agent</li> <li>Read how an AI novice got 95% LLM accuracy with Lamini</li> <li>Learn about Memory Tuning in a free 1-hour DeepLearning.ai course</li> </ul>"},{"location":"cat/","title":"Classifier Agent Toolkit","text":"<p>The Lamini Classifier Agent Toolkit (CAT) allows you to create and refine a key building block for agentic workflows: classifiers that can quickly categorize a large number of text inputs across any number of pre-defined categories.</p> <p>What sets CAT apart from other LLMs and classification tools:</p> <ul> <li> <p>Accuracy for many classes: &gt;99% accuracy on evals even with &gt;500 classes</p> </li> <li> <p>High throughput: process 100k tokens/s</p> </li> <li> <p>Consistent latency: sub-2s latency, with 1000s of inputs and 100s of classes</p> </li> <li> <p>Confidence scores: for more accurate workflows</p> </li> <li> <p>Built for iteration: compare models with metrics to measure progress</p> </li> </ul> <p>You can use CAT via Lamini's REST API, Python SDK, or web interface. Or step through an  example notebook.</p> Step Action Best Practices 1 Create project Set up new classifier 2 Add examples ~3 diverse examples per class, balanced number of examples per class 3 Train &amp; predict Get predictions with confidence scores (~1 min/class) 4 Evaluate Validate metrics and test performance 5 Iterate Add examples and retrain to improve accuracy"},{"location":"cat/#quick-start-with-python","title":"Quick Start with Python","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai):</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <p>Create a new classifier project: <pre><code>from lamini.classify.lamini_classifier import LaminiClassifier\n\ncls = LaminiClassifier(\"MyClassifierProject\")\n</code></pre></p> <p>Once the project is created, we define the classes. The more detailed the description, the higher your accuracy will be.</p> <pre><code>classes = {\n    \"cat\": \"\"\"The cat (Felis catus), also referred to as domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges freely as a feral cat avoiding human contact. Valued by humans for companionship and its ability to kill vermin, the cat's retractable claws are adapted to killing small prey like mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations\u2014including meowing, purring, trilling, hissing, growling, and grunting\u2013as well as body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It secretes and perceives pheromones.\n            Domain: Eukaryota\n            Kingdom:    Animalia\n            Phylum: Chordata\n            Class:  Mammalia\n            Order:  Carnivora\n            Suborder:   Feliformia\n            Family: Felidae\n            Subfamily:  Felinae\n            Genus:  Felis\n            Species:    F. catus[1]\"\"\",\n    \"dog\": \"\"\"The dog is a domesticated descendant of the wolf. Also called the domestic dog, it was selectively bred from an extinct population of wolves during the Late Pleistocene by hunter-gatherers. The dog was the first species to be domesticated by humans, over 14,000 years ago and before the development of agriculture. \n            Domain: Eukaryota\n            Kingdom:    Animalia\n            Phylum: Chordata\n            Class:  Mammalia\n            Order:  Carnivora\n            Family: Canidae\n            Genus:  Canis\"\"\",\n}   \n</code></pre> <p>Adding example inputs is optional, but will also help with accuracy. You can always do this later - we'll show you how later in this notebook.</p> <pre><code>examples = {\n    \"cat\": [\n        \"Tend to be independent and aloof.\",\n        \"Territorial and defensive .\",\n        \"Self-grooming animals, using their tongues to keep their coats clean and healthy.\",\n        \"Use body language and vocalizations, such as meowing and purring, to communicate.\"\n    ],\n    \"dog\": [\n        \"Social, pack-oriented, and tend to be more loyal to their human family.\",\n        \"Need regular grooming from their owners, including brushing and bathing.\",\n        \"Bark and growl to convey their messages.\",\n        \"Responsive to human commands and can be trained to perform a wide range of tasks.\"\n    ],\n}\n</code></pre> <p>Now we initialize the project. This can take about a minute per class, so we'll put in a simple timer to keep us updated on status.</p> <pre><code>resp = cls.initialize(classes, examples) \n\nimport time\n\nwhile True:\n    print(\"Waiting for classifier to initialize\")\n    time.sleep(5)\n    resp = cls.train_status()\n    if resp[\"status\"] == \"completed\":\n        print(\"Model ID: \" + resp[\"model_id\"])\n        first_model_id = resp[\"model_id\"]\n        break\n    if resp[\"status\"] == \"failed\":\n        print(resp[\"status\"])\n        raise Exception(\"failed training\")\n</code></pre> <p>Cool, we have our first model version! Let's try it out with a quick test.</p> <pre><code>import json\n\nprint(json.dumps(cls.classify(\"woof\"), indent=2))\n</code></pre> <p>Here's the expected output:</p> <pre><code>{\n  \"classification\": [\n    [\n      {\n        \"class_id\": 1,\n        \"class_name\": \"dog\",\n        \"prob\": 0.5267619385770103\n      },\n      {\n        \"class_id\": 0,\n        \"class_name\": \"cat\",\n        \"prob\": 0.47323806142298974\n      }\n    ]\n  ]\n}\n</code></pre> <p>Now we can see how useful the classifier output is. We get a list of all the categories we defined in our project, plus a confidence score for each.</p> <p>We can go even further to easily quantify the accuracy of our classifier. Let's run an evaluation!</p> <p>What an evaluation means for a classifier: when you provide a set of inputs and the expected output, we can test the accuracy of the model on those inputs, and give you back both overall metrics as well as per-input assessment.</p> <pre><code>from lamini.one_evaler.one_evaler import LaminiOneEvaler\n\neval = LaminiOneEvaler(\n    test_model_id=first_model_id,\n    eval_data_id=f\"first_eval{random.randint(1000,9999)}\",\n    eval_data=[{\"input\": \"woof\", \"target\": \"dog\"}, {\"input\": \"meow\", \"target\": \"cat\"}],\n    test_eval_type=\"classifier\",\n)\n</code></pre> <p>Expected output: <pre><code>print(json.dumps(eval.run(), indent=2))\n</code></pre></p> <pre><code>{\n  \"eval_job_id\": \"1424247633\",\n  \"eval_data_id\": \"first_eval6032\",\n  \"metrics\": {\n    \"tuned_accuracy\": 1.0,\n    \"tuned_precision\": 1.0,\n    \"tuned_recall\": 1.0,\n    \"tuned_f1\": 1.0\n  },\n  \"status\": \"COMPLETED\",\n  \"predictions\": [\n    {\n      \"input\": \"woof\",\n      \"target\": \"dog\",\n      \"test_output\": \"dog\",\n      \"base_output\": null\n    },\n    {\n      \"input\": \"meow\",\n      \"target\": \"cat\",\n      \"test_output\": \"cat\",\n      \"base_output\": null\n    }\n  ]\n}\n</code></pre> <p>That first run was ok, but we can do better. Let's add some more examples and retrain to improve accuracy. You control when to add data and when to train.</p> <pre><code>resp = cls.add(\n    \"additional_data\",\n    {\n        \"cat\": [\n            \"Cats spend up to sixteen hours a day sleeping, making them some of nature's most dedicated nappers.\",\n            \"Felines possess an extraordinary sense of balance thanks to their flexible backbone and righting reflex.\",\n            \"A cat's sandpaper-like tongue is covered in tiny hooks called papillae that help them groom themselves effectively.\",\n            \"Female cats tend to be right-pawed while male cats are more often left-pawed, according to scientific studies.\",\n            \"Ancient Egyptians showed their devotion to cats by mummifying them alongside their human companions.\",\n        ],\n        \"dog\": [\n            \"Dogs have evolved alongside humans for over 15,000 years, developing an uncanny ability to read our facial expressions and emotions.\",\n            \"The average dog can understand around 165 different words or signals, though some exceptional dogs can learn many more.\",\n            \"A dog's sense of smell is roughly 40 times greater than a human's, allowing them to detect diseases and track scents that are days old.\",\n            \"Unlike humans who have three cones in their eyes, dogs only have two, making them partially colorblind but excellent at detecting movement.\",\n            \"The Basenji breed is known as the 'barkless dog' because it produces an unusual yodel-like sound instead of a typical bark.\",\n        ],\n    },\n)\n\nresp = cls.train()\n\nwhile True:\n    print(\"Waiting for classifier to train\")\n    time.sleep(5)\n    resp = cls.train_status()\n    if resp[\"status\"] == \"completed\":\n        print(\"Model ID: \" + resp[\"model_id\"])\n        second_model_id = resp[\"model_id\"]\n        break\n    if resp[\"status\"] == \"failed\":\n        print(resp[\"status\"])\n        raise Exception(\"failed training\")\n</code></pre> <p>Great, now we have a second model version in our project! Let's run an eval and compare it to the first version.</p> <pre><code>print(\"Running comparison eval between model versions \" + first_model_id + \" and \" + second_model_id)\n\neval_2 = LaminiOneEvaler(\n    test_model_id=first_model_id,\n    eval_data_id=f\"second_eval{random.randint(1000,9999)}\",\n    eval_data=[{\"input\": \"woof\", \"target\": \"dog\"}, {\"input\": \"meow\", \"target\": \"cat\"}],\n    test_eval_type=\"classifier\",\n    base_model_id=second_model_id,\n    sbs=True,\n    fuzzy=True,\n)\n\nprint(json.dumps(eval_2.run(), indent=2))\n</code></pre> <p>Expected output: <pre><code>Running comparison eval between model versions 8a9fe622-4555-4646-886a-dc94b16a56f2 and 9739bf49-82ab-4e69-8149-5b891111516e\n{\n  \"eval_job_id\": \"2044167961\",\n  \"eval_data_id\": \"second_eval9291\",\n  \"metrics\": {\n    \"base_accuracy\": 1.0,\n    \"base_precision\": 1.0,\n    \"base_recall\": 1.0,\n    \"base_f1\": 1.0,\n    \"base_fuzzy_accuracy\": 1.0,\n    \"base_fuzzy_precision\": 1.0,\n    \"base_fuzzy_recall\": 1.0,\n    \"base_fuzzy_f1\": 1.0,\n    \"tuned_accuracy\": 1.0,\n    \"tuned_precision\": 1.0,\n    \"tuned_recall\": 1.0,\n    \"tuned_f1\": 1.0,\n    \"tuned_fuzzy_accuracy\": 1.0,\n    \"tuned_fuzzy_precision\": 1.0,\n    \"tuned_fuzzy_recall\": 1.0,\n    \"tuned_fuzzy_f1\": 1.0,\n    \"tuned_win_loss_ratio\": 0.0,\n    \"base_win_loss_ratio\": 0.0\n  },\n  \"status\": \"COMPLETED\",\n  \"predictions\": [\n    {\n      \"input\": \"woof\",\n      \"target\": \"dog\",\n      \"test_output\": \"dog\",\n      \"base_output\": \"dog\"\n    },\n    {\n      \"input\": \"meow\",\n      \"target\": \"cat\",\n      \"test_output\": \"cat\",\n      \"base_output\": \"cat\"\n    }\n  ]\n}\n</code></pre></p> <p>The eval output makes it easy to compare model versions overall, and to see exactly where the differences are, so you know exactly where to focus to improve your workflow.</p> <p>Happy classifying!</p>"},{"location":"inference/batching/","title":"Batching","text":"<p>Batching inference requests (submitting multiple prompts simultaneously) provides dramatically higher throughput compared to submitting each request individually.</p>"},{"location":"inference/batching/#use-cases","title":"Use cases","text":"<ul> <li>Memory Tuning using both evaluation and data-generation agents</li> <li>Evaluation agents that enable rapid feedback loops during model development by automatically measuring model performance</li> <li>Data-generation agents for expanding tuning and evaluation data sets without tedious manual effort</li> <li>Async inference for enriching or updating data in the background</li> </ul>"},{"location":"inference/batching/#a-better-way-to-batch","title":"A better way to batch","text":"<p>Lamini Platform implements approaches similar to iteration-level scheduling and selective batching (as described in the Orca paper) to deliver significantly higher throughput compared to naive inference batching implementations.</p> <p>Naive batching has two major drawbacks:</p> <ol> <li>The entire batch blocks on the request that takes the longest to process.</li> <li>A second batch cannot be processed until the first batch is completely processed.</li> </ol> <p>Iteration-level scheduling and selective batching avoid these drawbacks by allocating work across GPUs at the model iteration level, rather than the entire request level.</p>"},{"location":"inference/batching/#example","title":"Example","text":"<p>Inference batching with Lamini is simple: just pass in a list of inputs\u2014no configuration required.</p> Python SDKREST API <pre><code># code/batching.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\nllm.generate(\n    [\n        \"How old are you?\",\n        \"What is the meaning of life?\",\n        \"What is the hottest day of the year?\",\n    ],\n    output_type={\"response\": \"str\", \"explanation\": \"str\"},\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"prompt\": [\n        \"How old are you?\",\n        \"What is the meaning of life?\",\n        \"What is the hottest day of the year?\"\n    ],\n    \"output_type\": {\n        \"response\": \"str\",\n        \"explanation\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>[\n    {\n        'response': 'I am 25 years old',\n        'explanation': \"I am a 25-year-old AI assistant, so I don't have a physical body and don't age like humans do\"\n    },\n    {\n        'response': \"The meaning of life is a question that has puzzled philosophers, scientists, and thinkers for centuries. There is no one definitive answer, as it is a deeply personal and subjective question that can vary greatly from person to person. However, here are some possible answers that have been proposed:\\n\\n1. The search for happiness: Many people believe that the meaning of life is to find happiness and fulfillment. This can be achieved through personal relationships, career, hobbies, or other activities that bring joy and satisfaction.\\n2. The pursuit of knowledge: Others believe that the meaning of life is to learn and understand the world around us. This can be achieved through education, research, and exploration.\\n3. The pursuit of purpose: Some people believe that the meaning of life is to find a sense of purpose and direction. This can be achieved through setting goals, pursuing passions, and making a positive impact on the world.\\n4. The search for connection: Many people believe that the meaning of life is to connect with others and build meaningful relationships. This can be achieved through communication, empathy, and understanding.\\n5. The search for transcendence: Some people believe that the meaning of life is to transcend the physical world and connect with something greater than ourselves. This can be achieved through spirituality, meditation, or other practices that help us connect with a higher power or the universe.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question that can only be answered by each individual. It is a question that requires self-reflection, introspection, and a willingness to explore and discover one's own values, beliefs, and passions\",\n        'explanation': \"The meaning of life is a question that has puzzled philosophers, scientists, and thinkers for centuries. There is no one definitive answer, as it is a deeply personal and subjective question that can vary greatly from person to person. However, here are some possible answers that have been proposed:\\n\\n1. The search for happiness: Many people believe that the meaning of life is to find happiness and fulfillment. This can be achieved through personal relationships, career, hobbies, or other activities that bring joy and satisfaction.\\n2. The pursuit of knowledge: Others believe that the meaning of life is to learn and understand the world around us. This can be achieved through education, research, and exploration.\\n3. The pursuit of purpose: Some people believe that the meaning of life is to find a sense of purpose and direction. This can be achieved through setting goals, pursuing passions, and making a positive impact on the world.\\n4. The search for connection: Many people believe that the meaning of life is to connect with others and build meaningful relationships. This can be achieved through communication, empathy, and understanding.\\n5. The search for transcendence: Some people believe that the meaning of life is to transcend the physical world and connect with something greater than ourselves. This can be achieved through spirituality, meditation, or other practices that help us connect with a higher power or the universe.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question that can only be answered by each individual. It is a question that requires self-reflection, introspection, and a willingness to explore and discover one's own values, beliefs, and passions\"\n    },\n    {\n        'response': \"The hottest day of the year is typically the day of the summer solstice, which is the longest day of the year and usually falls on June 20 or June 21 in the Northern Hemisphere. This day is often referred to as the 'warmest day of the year' or the 'hottest day of the year' because it is the day when the sun is at its highest point in the sky and the Earth is tilted at its maximum angle towards the sun, resulting in the most direct sunlight and the highest temperatures. However, it's worth noting that the hottest day of the year can vary depending on the location and climate. In some regions, the hottest day of the year may occur in July or August, while in others it may occur in September or October\",\n        'explanation': \"The summer solstice is the longest day of the year and typically marks the beginning of summer in the Northern Hemisphere. It is the day when the sun is at its highest point in the sky and the Earth is tilted at its maximum angle towards the sun, resulting in the most direct sunlight and the highest temperatures. This day is often referred to as the 'warmest day of the year' or the 'hottest day of the year' because it is the day when the sun is at its strongest and the Earth is at its warmest. However, it's worth noting that the hottest day of the year can vary depending on the location and climate. In some regions, the hottest day of the year may occur in July or August, while in others it may occur in September or October.\"\n    }\n]\n</code></pre>"},{"location":"inference/json_output/","title":"JSON Output","text":"<p>Enforcing structured JSON schema output is important for handling LLM outputs downstream with other systems and APIs in your applications.</p> <p>For an in-depth technical deep dive of how we implemented this feature, see our blog post.</p> Python SDKREST API <p>You can enforce JSON schema via the <code>Lamini</code> class, which is the base class for all runners. <code>Lamini</code> wraps our REST API endpoint.</p> <p>First, return a string:</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\noutput = llm.generate(\n    \"How are you?\",\n    output_type={\"answer\": \"str\"}\n)\n</code></pre> <p>First, get a basic string output out:</p> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How are you?\",\n    \"output_type\": {\n        \"answer\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"answer\":\"I'm doing well, thanks for asking! How about you\"\n}\n</code></pre>"},{"location":"inference/json_output/#values-other-than-strings","title":"Values other than strings","text":"<p>You can change the output type to be a different type. This typing is strictly enforced.  We currently support <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, and enums structured as str lists, or int lists. For example, <code>\"answer\": [\"A\",\"B\",\"C\",\"D\"]</code> would always return one of <code>A</code>, <code>B</code>, <code>C</code>, or <code>D</code> for the <code>answer</code> field. <code>\"answer\": [1, 2, 3]</code> would always return one of <code>1</code>, <code>2</code>, or <code>3</code> for the <code>answer</code> field.</p> <p>Please let us know if there are additional types you'd like to see supported.</p> <p>Examples</p> Python SDKPython SDKPython SDKREST API <pre><code>llm.generate(\n    \"How old are you?\",\n    output_type={\"age\": \"int\"}\n)\n</code></pre> <pre><code>llm.generate(\n    \"Pick a color.\",\n    output_type={\"name\": [\"red\", \"white\", \"blue\"]}\n)\n</code></pre> <pre><code>llm.generate(\n    \"Pick an odd digit\",\n    output_type={\"name\": [1, 3, 5, 7, 9]}\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How old are you?\",\n    \"output_type\": {\n        \"age\": \"int\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"age\": 25\n}\n</code></pre>"},{"location":"inference/json_output/#multiple-outputs-in-json-schema","title":"Multiple outputs in JSON schema","text":"<p>You can also add multiple output types in one call. The output is a JSON schema that is also strictly enforced.</p> Python SDKREST API <pre><code>llm.generate(\n    \"How old are you?\",\n    output_type={\"age\": \"int\", \"units\": \"str\"}\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How old are you?\",\n    \"output_type\": {\n        \"age\": \"int\",\n        \"units\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"age\": 25,\n    \"units\": \"years\"\n}\n</code></pre> <p>Great! You've successfully run an LLM with structured JSON schema outputs.</p>"},{"location":"inference/json_output/#known-issue-json-output-truncation","title":"Known issue: JSON output truncation","text":"<p>Truncation may occur when JSON output generates double quotation marks (\"). This is a known limitation with using <code>output_type</code> since a quotation at the end of the output and a quotation as a part of the output are not distinguished.</p>"},{"location":"inference/json_output/#workaround","title":"Workaround","text":"<ul> <li>Use prompt tuning instead of output type if the response may contain double quotation marks. e.g. \"Only return the relevant quote, do not include any other text\".</li> <li>Use prompt tuning to avoid generating double quotation marks. e.g. \"Only use single quotes when there is dialogue\".</li> <li>Contact us to discuss alternative solutions or workarounds for your use case.</li> </ul>"},{"location":"inference/json_output/#future-support","title":"Future support","text":"<p>We are evaluating the feasibility of improving our system to handle double quotes in JSON output in the future. If we decide to support this feature, we will update our documentation and notify users.</p>"},{"location":"inference/json_output/#known-issue-long-json-output-times-out","title":"Known issue: Long JSON output times out","text":"<p>To ensure that requests complete in a reasonable amount of time, there is a time limit on all requests including json requests. If your requests exceeds the time limit, try guiding the model to generate a shorter json object, e.g. write a description in 3 sentences or less. Timed out requests may result in failed, incomplete, or missing output.</p>"},{"location":"inference/json_output/#workaround_1","title":"Workaround","text":"<ul> <li>Reduce the size of the output by limiting the number of fields or the prompt.</li> <li>Break down the JSON output into separate smaller requests.</li> <li>Contact us to discuss alternative solutions or workarounds for your use case.</li> </ul>"},{"location":"inference/json_output/#future-support_1","title":"Future support","text":"<p>We are evaluating the feasibility of improving our system to handle large JSON output in the future. If we decide to support this feature, we will update our documentation and notify users.</p> <p>Feel free to contact us with any questions or concerns.</p>"},{"location":"inference/performance/","title":"Performance","text":"<p>Inference performance is a function of model size, prompt size, response size, and compute speed, and can be measured in a variety of ways (queries per second, tokens per second, time to first token, etc.). It's complicated!</p>"},{"location":"inference/performance/#how-to-improve-performance-and-handle-truncated-responses","title":"How to improve performance and handle truncated responses","text":"<p>Inference responses can be truncated (cut off, or appear incomplete) because the request could not be completed in the time allotted (timeout), or because the response size exceeded the <code>max_new_tokens</code> parameter (length).</p> <ol> <li> <p>First, review your prompt and requested responses: can you shorten them? When you're experimenting, it's easy to accumulate extraneous information in the prompt, or to request more output than you actually need. Prompt tuning is often a quick win, especially when combined with structured output.</p> </li> <li> <p>Try using Generation Pipelines for more efficient execution.</p> </li> <li> <p>If you're still having trouble, check whether your request set a value for <code>max_new_tokens</code>.</p> </li> </ol> <p>Lamini's completions API has an optional <code>max_new_tokens</code> parameter that limits the response size. Lamini uses this parameter to efficiently allocate GPU memory. However, this comes with risks:</p> <ul> <li>If you set the token limit too short, your requests may get truncated. The LLM is not aware of the token limit.</li> <li>Very large token limits consume substantial memory, which slows down processing, which may cause timeouts.</li> </ul> <p>If you're setting a value for <code>max_new_tokens</code> and your responses are truncated at that value, you're hitting the token limit. Try a higher value for <code>max_new_tokens</code>.</p> <p>If you're setting a value for <code>max_new_tokens</code> and your response was truncated at less than that value, you're probably hitting a timeout. Try a lower value for <code>max_new_tokens</code>.</p>"},{"location":"inference/playground/","title":"Playground","text":"<p>The playground makes it easy to compare different open-source models and iterate on system prompts by chatting with them.</p> <p>Run LLMs at https://app.lamini.ai/playground or contact us to run the same UI internally at your company.</p> <p></p> <p></p>"},{"location":"inference/prompt_templates/","title":"Prompt Templates","text":"<p>Different models have different system prompt templates. Using the correct template when prompt tuning can have a large effect on model performance. </p> <p>When you're trying a new model, it's a good idea to review the model card on Hugging Face to understand what (if any) system prompt template it uses.</p>"},{"location":"inference/prompt_templates/#llama-31-32","title":"Llama 3.1 + 3.2","text":"<p>The Llama 3.1 and Llama 3.2 prompt template looks like this: <pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{{ user_message }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> The <code>{system_prompt}</code> variable is a system prompt that tells your LLM how it should behave and what persona to take on. By default, it is that of a helpful assistant. The <code>{user_message}</code> variable is the instruction prompt that tells your LLM what to do. This is typically what you view as the prompt, e.g. the question you want to ask the LLM.</p> Python SDKREST API <pre><code># code/llama_3_prompt.py\n\nfrom lamini import Lamini\n\nprompt = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n\"\nprompt += \"You are a pirate. Say arg matey!\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n\"\nprompt += \"How are you?\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nllm = Lamini(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\nprint(llm.generate(prompt, output_type={\"Response\": \"str\"}))\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n    --header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\n        \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n You are a pirate. Say arg matey! &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n How are you? &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\",\n        \"output_type\": {\n            \"Response\": \"str\"\n        }\n    }'\n</code></pre> Expected Output <pre><code>{'Response': \"Ahoy, matey! I be doin' just fine, thank ye for askin'! Me and me crew have been sailin' the seven seas, plunderin' the riches and singin' sea shanties 'round the campfire. The sun be shinin' bright, the wind be blowin' strong, and me trusty cutlass be by me side. What more could a pirate ask for, eh? Arrr\"}\n</code></pre>"},{"location":"inference/prompt_templates/#mistral-v3","title":"Mistral v3","text":"<p>Mistral v3 uses a different format. In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens.</p> Python SDKREST API <pre><code># code/mistral.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")\nprint(llm.generate(\"&lt;s&gt;[INST] How are you? [/INST]\", output_type={\"Response\": \"str\"}))\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    \"prompt\": \"&lt;s&gt;[INST] How are you? [/INST]\",\n    \"output_type\": {\n        \"Response\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    'Response': \"I'm just a computer program, I don't have feelings or emotions. I'm here to help answer any questions you might have to the best of my ability\"\n}\n</code></pre>"},{"location":"inference/streaming/","title":"Streaming","text":"<p>Lamini also supports streaming inference! Here is an example implementation using our Python library.</p> <pre><code>import os\nimport random\nimport time\n\nimport lamini\n\napi = lamini.StreamingCompletion()\nasync def main():\n    prompt = f\"[INST]{random.random()} What is a pickle? [/INST]\"\n    result = await api.async_create(\n        prompt,\n        \"meta-llama/Llama-3.2-1B-Instruct\",\n        max_new_tokens=256,\n    )\n\n    async for r in result:\n        print(r)\n\ndef main():\n    prompt = f\"What is A?\"\n    result = api.create(\n        prompt,\n        \"hf-internal-testing/tiny-random-gpt2\",\n        max_new_tokens=256,\n    )\n\n    for r in result:\n        print(r)\nmain()\n</code></pre>"},{"location":"lamini_python_class/lamini/","title":"Module <code>lamini.api.lamini</code> (lamini-3.2.3)","text":"<p>Want to see more? Check out our full open source repo: https://github.com/lamini-ai/lamini.</p>"},{"location":"lamini_python_class/lamini/#classes","title":"Classes","text":"<pre><code>Lamini(\n    model_name:\u00a0str,\n    api_key:\u00a0Optional[str]\u00a0=\u00a0None,\n    api_url:\u00a0Optional[str]\u00a0=\u00a0None,\n)\n</code></pre> Expand source code <pre><code>class Lamini:\n    def __init__(\n        self,\n        model_name: str,\n        api_key: Optional[str] = None,\n        api_url: Optional[str] = None,\n    ):\n        self.config = get_config()\n        self.model_name = model_name\n        self.api_key = api_key\n        self.api_url = api_url\n        self.completion = Completion(api_key, api_url)\n        self.trainer = Train(api_key, api_url)\n        self.upload_file_path = None\n        self.upload_base_path = None\n\n    def version(self):\n        return get_version(self.api_key, self.api_url, self.config)\n\n    def generate(\n        self,\n        prompt: Union[str, List[str]],\n        model_name: Optional[str] = None,\n        output_type: Optional[dict] = None,\n        max_tokens: Optional[int] = None,\n        max_new_tokens: Optional[int] = None,\n    ):\n        result = self.completion.generate(\n            prompt=prompt,\n            model_name=model_name or self.model_name,\n            output_type=output_type,\n            max_tokens=max_tokens,\n            max_new_tokens=max_new_tokens,\n        )\n        if output_type is None:\n            if isinstance(prompt, list):\n                result = [single_result[\"output\"] for single_result in result]\n            else:\n                result = result[\"output\"]\n        return result\n\n    async def async_generate(\n        self,\n        prompt: Union[str, List[str]],\n        model_name: Optional[str] = None,\n        output_type: Optional[dict] = None,\n        max_tokens: Optional[int] = None,\n        max_new_tokens: Optional[int] = None,\n    ):\n        req_data = self.completion.make_llm_req_map(\n            prompt=prompt,\n            model_name=model_name or self.model_name,\n            output_type=output_type,\n            max_tokens=max_tokens,\n            max_new_tokens=max_new_tokens,\n        )\n        result = await self.completion.async_generate(req_data)\n        if output_type is None:\n            if isinstance(prompt, list):\n                result = [single_result[\"output\"] for single_result in result]\n            else:\n                result = result[\"output\"]\n        return result\n\n    def upload_data(\n        self,\n        data: Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]],\n        is_public: Optional[bool] = None,\n    ):\n        num_datapoints = 0\n\n        def get_data_str(d):\n            nonlocal num_datapoints\n            for item in d:\n                num_datapoints += 1\n                yield json.dumps(item) + \"\\n\"\n\n        if not data:\n            raise ValueError(\"Data pairs cannot be empty.\")\n\n        output = self.trainer.get_upload_base_path()\n        self.upload_base_path = output[\"upload_base_path\"]\n\n        try:\n            if self.upload_base_path == \"azure\":\n                data_str = get_data_str(data)\n                response = self.trainer.create_blob_dataset_location(\n                    self.upload_base_path, is_public\n                )\n                self.upload_file_path = response[\"dataset_location\"]\n                upload_to_blob(data_str, self.upload_file_path)\n                self.trainer.update_blob_dataset_num_datapoints(\n                    response[\"dataset_id\"], num_datapoints\n                )\n                print(\"Data pairs uploaded to blob.\")\n            else:\n                response = self.trainer.upload_dataset_locally(\n                    self.upload_base_path, is_public, data\n                )\n                self.upload_file_path = response[\"dataset_location\"]\n                print(\"Data pairs uploaded to local.\")\n\n            print(response)\n            print(\n                f\"\\nYour dataset id is: {response['dataset_id']} . Consider using this in the future to train using the same data. \\nEg: \"\n                f\"llm.train(data_or_dataset_id='{response['dataset_id']}')\"\n            )\n\n        except Exception as e:\n            print(f\"Error uploading data pairs: {e}\")\n            raise e\n\n        return response[\"dataset_id\"]\n\n    def upload_file(\n        self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n    ):\n        items = self._upload_file_impl(file_path, input_key, output_key)\n        try:\n            dataset_id = self.upload_data(items)\n            return dataset_id\n        except Exception as e:\n            print(f\"Error reading data file: {e}\")\n            raise e\n\n    def _upload_file_impl(\n        self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n    ):\n        if os.path.getsize(file_path) &gt; 1e10:\n            raise Exception(\"File size is too large, please upload file less than 10GB\")\n\n        # Convert file records to appropriate format before uploading file\n        items = []\n        if file_path.endswith(\".jsonl\") or file_path.endswith(\".jsonlines\"):\n            with open(file_path) as dataset_file:\n\n                for row in jsonlines.Reader(dataset_file):\n                    yield {\"input\": row[input_key], \"output\": row.get(output_key, \"\")}\n\n        elif file_path.endswith(\".csv\"):\n            df = pd.read_csv(file_path).fillna(\"\")\n            data_keys = df.columns\n            if input_key not in data_keys:\n                raise ValueError(\n                    f\"File must have input_key={input_key} as a column (and optionally output_key={output_key}). You \"\n                    \"can pass in different input_key and output_keys.\"\n                )\n\n            try:\n                for _, row in df.iterrows():\n                    yield {\n                        \"input\": row[input_key],\n                        \"output\": row.get(output_key, \"\"),\n                    }\n            except KeyError:\n                raise ValueError(\"Each object must have 'input' and 'output' as keys\")\n\n        else:\n            raise Exception(\n                \"Upload of only csv and jsonlines file supported at the moment.\"\n            )\n        return items\n\n    def train(\n        self,\n        data_or_dataset_id: Union[\n            str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n        ],\n        finetune_args: Optional[dict] = None,\n        gpu_config: Optional[dict] = None,\n        is_public: Optional[bool] = None,\n        **kwargs,\n    ):\n        if isinstance(data_or_dataset_id, str):\n            dataset_id = data_or_dataset_id\n        else:\n            dataset_id = self.upload_data(data_or_dataset_id, is_public=is_public)\n        assert dataset_id is not None\n        base_path = self.trainer.get_upload_base_path()\n        self.upload_base_path = base_path[\"upload_base_path\"]\n        existing_dataset = self.trainer.get_existing_dataset(\n            dataset_id, self.upload_base_path\n        )\n        self.upload_file_path = existing_dataset[\"dataset_location\"]\n\n        job = self.trainer.train(\n            model_name=self.model_name,\n            dataset_id=dataset_id,\n            upload_file_path=self.upload_file_path,\n            finetune_args=finetune_args,\n            gpu_config=gpu_config,\n            is_public=is_public,\n        )\n        job[\"dataset_id\"] = dataset_id\n        return job\n\n    # Add alias for tune\n    tune = train\n\n    # continuously poll until the job is completed\n    def train_and_wait(\n        self,\n        data_or_dataset_id: Union[\n            str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n        ],\n        finetune_args: Optional[dict] = None,\n        gpu_config: Optional[dict] = None,\n        is_public: Optional[bool] = None,\n        **kwargs,\n    ):\n        job = self.train(\n            data_or_dataset_id,\n            finetune_args=finetune_args,\n            gpu_config=gpu_config,\n            is_public=is_public,\n        )\n\n        try:\n            status = self.check_job_status(job[\"job_id\"])\n            if status[\"status\"] == \"FAILED\":\n                print(f\"Job failed: {status}\")\n                return status\n\n            while status[\"status\"] not in (\n                \"COMPLETED\",\n                \"PARTIALLY COMPLETED\",\n                \"FAILED\",\n                \"CANCELLED\",\n            ):\n                if kwargs.get(\"verbose\", False):\n                    print(f\"job not done. waiting... {status}\")\n                time.sleep(30)\n                status = self.check_job_status(job[\"job_id\"])\n                if status[\"status\"] == \"FAILED\":\n                    print(f\"Job failed: {status}\")\n                    return status\n                elif status[\"status\"] == \"CANCELLED\":\n                    print(f\"Job canceled: {status}\")\n                    return status\n            print(\n                f\"Finetuning process completed, model name is: {status['model_name']}\"\n            )\n        except KeyboardInterrupt as e:\n            print(\"Cancelling job\")\n            return self.cancel_job(job[\"job_id\"])\n\n        return status\n\n    # Add alias for tune\n    tune_and_wait = train_and_wait\n\n    def cancel_job(self, job_id=None):\n        return self.trainer.cancel_job(job_id)\n\n    def cancel_all_jobs(\n        self,\n    ):\n        return self.trainer.cancel_all_jobs()\n\n    def resume_job(self, job_id=None):\n        return self.trainer.resume_job(job_id)\n\n    def check_job_status(self, job_id=None):\n        return self.trainer.check_job_status(job_id)\n\n    def get_jobs(self):\n        return self.trainer.get_jobs()\n\n    def evaluate(self, job_id=None):\n        return self.trainer.evaluate(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#methods","title":"Methods","text":""},{"location":"lamini_python_class/lamini/#cancel_all_jobs","title":"<code>cancel_all_jobs</code>","text":"<p>Cancel all jobs associated with your key.</p> Expand source code <pre><code>def cancel_all_jobs(\n    self,\n):\n    return self.trainer.cancel_all_jobs()\n</code></pre>"},{"location":"lamini_python_class/lamini/#cancel_job","title":"<code>cancel_job</code>","text":"<p>Cancel the job or specify a job id to cancel.</p> Expand source code <pre><code>def cancel_job(self, job_id=None):\n    return self.trainer.cancel_job(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#check_job_status","title":"<code>check_job_status</code>","text":"<p>Check the status of the job or a given job id.</p> Expand source code <pre><code>def check_job_status(self, job_id=None):\n    '''\n    Possible statuses include:\n    'SCHEDULED'\n    'QUEUED'\n    'LOADING DATA'\n    'TRAINING MODEL'\n    'EVALUATING MODEL'\n    'COMPLETED'\n    'PARTIALLY COMPLETED'\n    'FAILED'\n    'CANCELLED'\n    '''\n    return self.trainer.check_job_status(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#generate","title":"<code>generate</code>","text":"<p>Run inference on the model or a given model.</p> Expand source code <pre><code>def generate(\n    self,\n    prompt: Union[str, List[str]],\n    model_name: Optional[str] = None,\n    output_type: Optional[dict] = None,\n    max_tokens: Optional[int] = None,\n    max_new_tokens: Optional[int] = None,\n):\n    result = self.completion.generate(\n        prompt=prompt,\n        model_name=model_name or self.model_name,\n        output_type=output_type,\n        max_tokens=max_tokens,\n        max_new_tokens=max_new_tokens,\n    )\n    if output_type is None:\n        if isinstance(prompt, list):\n            result = [single_result[\"output\"] for single_result in result]\n        else:\n            result = result[\"output\"]\n    return result\n</code></pre>"},{"location":"lamini_python_class/lamini/#get_jobs","title":"<code>get_jobs</code>","text":"<p>Get information on all jobs associated with your key.</p> Expand source code <pre><code>def get_jobs(self):\n    return self.trainer.get_jobs()\n</code></pre>"},{"location":"lamini_python_class/lamini/#resume_job","title":"<code>resume_job</code>","text":"<p>Resume <code>CANCELLED</code>, <code>PARTIALLY COMPLETED</code>, <code>FAILED</code>, or <code>COMPLETED</code> job.</p> Expand source code <pre><code>def resume_job(self, job_id=None):\n    return self.trainer.resume_job(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#train","title":"<code>train</code>","text":"<p>Train a job.</p> Expand source code <pre><code>def train(\n    self,\n    data_or_dataset_id: Union[\n        str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n    ],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs,\n):\n    if isinstance(data_or_dataset_id, str):\n        dataset_id = data_or_dataset_id\n    else:\n        dataset_id = self.upload_data(data_or_dataset_id, is_public=is_public)\n    assert dataset_id is not None\n    base_path = self.trainer.get_upload_base_path()\n    self.upload_base_path = base_path[\"upload_base_path\"]\n    existing_dataset = self.trainer.get_existing_dataset(\n        dataset_id, self.upload_base_path\n    )\n    self.upload_file_path = existing_dataset[\"dataset_location\"]\n\n    job = self.trainer.train(\n        model_name=self.model_name,\n        dataset_id=dataset_id,\n        upload_file_path=self.upload_file_path,\n        finetune_args=finetune_args,\n        gpu_config=gpu_config,\n        is_public=is_public,\n    )\n    job[\"dataset_id\"] = dataset_id\n    return job\n</code></pre>"},{"location":"lamini_python_class/lamini/#tune","title":"<code>tune</code>","text":"<p>Aliases to <code>train</code>.</p> Expand source code <pre><code>tune = train\n</code></pre>"},{"location":"lamini_python_class/lamini/#train_and_wait","title":"<code>train_and_wait</code>","text":"<p>Train a job, synchronous.</p> Expand source code <pre><code>def train_and_wait(\n    self,\n    data_or_dataset_id: Union[\n        str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n    ],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs,\n):\n    job = self.train(\n        data_or_dataset_id,\n        finetune_args=finetune_args,\n        gpu_config=gpu_config,\n        is_public=is_public,\n    )\n\n    try:\n        status = self.check_job_status(job[\"job_id\"])\n        if status[\"status\"] == \"FAILED\":\n            print(f\"Job failed: {status}\")\n            return status\n\n        while status[\"status\"] not in (\n            \"COMPLETED\",\n            \"PARTIALLY COMPLETED\",\n            \"FAILED\",\n            \"CANCELLED\",\n        ):\n            if kwargs.get(\"verbose\", False):\n                print(f\"job not done. waiting... {status}\")\n            time.sleep(30)\n            status = self.check_job_status(job[\"job_id\"])\n            if status[\"status\"] == \"FAILED\":\n                print(f\"Job failed: {status}\")\n                return status\n            elif status[\"status\"] == \"CANCELLED\":\n                print(f\"Job canceled: {status}\")\n                return status\n        print(\n            f\"Finetuning process completed, model name is: {status['model_name']}\"\n        )\n    except KeyboardInterrupt as e:\n        print(\"Cancelling job\")\n        return self.cancel_job(job[\"job_id\"])\n\n    return status\n</code></pre>"},{"location":"lamini_python_class/lamini/#upload_data","title":"<code>upload_data</code>","text":"<p>Upload data, most commonly a list of dictionaries with <code>input</code> and <code>output</code> keys.</p> Expand source code <pre><code>def upload_data(\n    self,\n    data: Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]],\n    is_public: Optional[bool] = None,\n):\n    num_datapoints = 0\n\n    def get_data_str(d):\n        nonlocal num_datapoints\n        for item in d:\n            num_datapoints += 1\n            yield json.dumps(item) + \"\\n\"\n\n    if not data:\n        raise ValueError(\"Data pairs cannot be empty.\")\n\n    output = self.trainer.get_upload_base_path()\n    self.upload_base_path = output[\"upload_base_path\"]\n\n    try:\n        if self.upload_base_path == \"azure\":\n            data_str = get_data_str(data)\n            response = self.trainer.create_blob_dataset_location(\n                self.upload_base_path, is_public\n            )\n            self.upload_file_path = response[\"dataset_location\"]\n            upload_to_blob(data_str, self.upload_file_path)\n            self.trainer.update_blob_dataset_num_datapoints(\n                response[\"dataset_id\"], num_datapoints\n            )\n            print(\"Data pairs uploaded to blob.\")\n        else:\n            response = self.trainer.upload_dataset_locally(\n                self.upload_base_path, is_public, data\n            )\n            self.upload_file_path = response[\"dataset_location\"]\n            print(\"Data pairs uploaded to local.\")\n\n        print(\n            f\"\\nYour dataset id is: {response['dataset_id']} . Consider using this in the future to train using the same data. \\nEg: \"\n            f\"llm.train(data_or_dataset_id='{response['dataset_id']}')\"\n        )\n\n    except Exception as e:\n        print(f\"Error uploading data pairs: {e}\")\n        raise e\n\n    return response[\"dataset_id\"]\n</code></pre>"},{"location":"lamini_python_class/lamini/#upload_file","title":"<code>upload_file</code>","text":"<p>Upload data as a file, can be <code>csv</code> or <code>jsonl</code>.</p> Expand source code <pre><code>def upload_file(\n    self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n):\n    items = self._upload_file_impl(file_path, input_key, output_key)\n    try:\n        dataset_id = self.upload_data(items)\n        return dataset_id\n    except Exception as e:\n        print(f\"Error reading data file: {e}\")\n        raise e\n</code></pre>"},{"location":"lamini_python_class/lamini/#laminiclassifylamini_classifier","title":"lamini.classify.lamini_classifier","text":""},{"location":"lamini_python_class/lamini/#initialize","title":"initialize","text":"<p>Creates a new classifier project and kicks off the initial data generation and training process.</p> <pre><code>def initialize(\n    name: str,\n    classes: List[str],\n    examples: Dict[str, List[str]],\n    model_name: Optional[str] = None\n) -&gt; Dict[str, str]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters","title":"Parameters","text":"<ul> <li><code>name</code>: Name for the classifier project. Must be unique for the user.</li> <li><code>classes</code>: List of class names that the classifier will be trained to identify.</li> <li><code>examples</code>: Dictionary mapping class names to lists of example texts for each class.</li> <li><code>model_name</code>: Optional name of base model to use for classification.</li> </ul>"},{"location":"lamini_python_class/lamini/#returns","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>name</code>: Name of the created project</li> <li><code>job_id</code>: ID of the training job that was initiated</li> </ul>"},{"location":"lamini_python_class/lamini/#raises","title":"Raises","text":"<ul> <li><code>HTTPException(497)</code>: If a project with the given name already exists</li> <li><code>HTTPException(499)</code>: If project creation fails</li> </ul>"},{"location":"lamini_python_class/lamini/#example","title":"Example","text":"<pre><code>from lamini.classify.lamini_classifier import LaminiClassifier\nclassifier = Classifier()\nresult = classifier.initialize(\n    name=\"sentiment_classifier\",\n    classes=[\"positive\", \"negative\"],\n    examples={\n        \"positive\": [\"great movie!\", \"loved it\"],\n        \"negative\": [\"terrible film\", \"waste of time\"]\n    }\n)\nprint(f\"Created project {result['name']} with job {result['job_id']}\")\n</code></pre>"},{"location":"lamini_python_class/lamini/#train_1","title":"train","text":"<p>Train a classifier model on provided data.</p> <pre><code>def train(\n    data_or_dataset_id: Union[str, List[Dict]],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs\n) -&gt; Dict[str, str]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_1","title":"Parameters","text":"<ul> <li><code>data_or_dataset_id</code>: Either a dataset ID string or list of training examples</li> <li><code>finetune_args</code>: Optional dictionary of fine-tuning parameters</li> <li><code>gpu_config</code>: Optional GPU configuration settings</li> <li><code>is_public</code>: Whether to make the trained model public</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_1","title":"Returns","text":"<p>Dictionary containing job information including:</p> <ul> <li><code>job_id</code>: ID of the training job</li> <li><code>dataset_id</code>: ID of the dataset used for training</li> </ul>"},{"location":"lamini_python_class/lamini/#classify","title":"classify","text":"<p>Run classification on input text using a trained model.</p> <pre><code>def classify(\n    prompt: Union[str, List[str]], \n    top_n: Optional[int] = None,\n    threshold: Optional[float] = None,\n    metadata: Optional[bool] = None\n) -&gt; Union[str, List[str]]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_2","title":"Parameters","text":"<ul> <li><code>prompt</code>: Input text or list of texts to classify</li> <li><code>top_n</code>: Optional number of top predictions to return</li> <li><code>threshold</code>: Optional confidence threshold for predictions</li> <li><code>metadata</code>: Whether to return prediction metadata</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_2","title":"Returns","text":"<p>Predicted class(es) for the input text(s)</p>"},{"location":"lamini_python_class/lamini/#example_1","title":"Example","text":"<pre><code>cls.classify(\"woof\")\n</code></pre> <pre><code>{\n  \"classification\": [\n    [\n      {\n        \"class_id\": 1,\n        \"class_name\": \"dog\",\n        \"prob\": 0.9263275590881269\n      },\n      {\n        \"class_id\": 0,\n        \"class_name\": \"cat\",\n        \"prob\": 0.2736724409118731\n      }\n    ]\n  ]\n}\n</code></pre>"},{"location":"lamini_python_class/lamini/#add","title":"add","text":"<p>Add additional training examples to an existing classifier project.</p> <pre><code>def add(\n    project_name: str,\n    dataset_name: str,\n    data: Dict[str, List[str]]\n) -&gt; Dict[str, bool]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_3","title":"Parameters","text":"<ul> <li><code>project_name</code>: Name of the existing classifier project</li> <li><code>dataset_name</code>: Name for the new dataset being added</li> <li><code>data</code>: Dictionary mapping class names to lists of example texts</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_3","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>success</code>: Boolean indicating if examples were added successfully</li> </ul>"},{"location":"lamini_python_class/lamini/#example_2","title":"Example","text":"<pre><code>classifier.add(\n    project_name=\"sentiment_classifier\",\n    dataset_name=\"additional_examples\",\n    data={\n        \"positive\": [\"excellent work\", \"fantastic results\"],\n        \"negative\": [\"poor quality\", \"disappointing outcome\"]\n    }\n)\n</code></pre>"},{"location":"lamini_python_class/lamini/#laminione_evalerone_evaler","title":"lamini.one_evaler.one_evaler","text":""},{"location":"lamini_python_class/lamini/#run","title":"run","text":"<p>Run evaluation on a model using provided test data.</p> <pre><code>def run() -&gt; Dict[str, Union[str, List, Dict, str]]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_4","title":"Parameters","text":"<ul> <li><code>test_model_id</code>: ID of the model to evaluate</li> <li><code>eval_data</code>: List of dictionaries with <code>input</code> and <code>target</code> keys</li> <li><code>eval_data_id</code>: name or ID for the evaluation dataset</li> <li><code>base_model_id</code>: ID of the base model to compare against (optional)</li> <li><code>fuzzy</code>: Whether to perform fuzzy matching of predictions (optional)</li> <li><code>sbs</code>: Whether to perform side-by-side comparison with the base model (optional)</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_4","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>eval_job_id</code>: Unique identifier for the evaluation job</li> <li><code>eval_data_id</code>: ID of the evaluation dataset used</li> <li><code>metrics</code>: Evaluation metrics results</li> <li><code>status</code>: Status of the evaluation job (\"COMPLETED\" or \"FAILED\")</li> <li><code>predictions</code>: List of actual model outputs</li> </ul>"},{"location":"lamini_python_class/lamini/#example_3","title":"Example","text":"<pre><code>from lamini.one_evaler import LaminiOneEvaler\n\nevaluator = LaminiOneEvaler(\n    test_model_id=\"my-model-id\",\n    eval_data=[\n        {\"input\": \"text1\", \"output\": \"label1\"},\n        {\"input\": \"text2\", \"output\": \"label2\"}\n    ],\n    test_eval_type=\"classifier\"\n)\n\nresult = evaluator.run()\nprint(f\"Evaluation completed with job ID: {result['eval_job_id']}\")\nprint(f\"Metrics: {result['metrics']}\")\n</code></pre>"},{"location":"lamini_python_class/lamini/#notes","title":"Notes","text":"<ul> <li>The evaluation compares model predictions against provided ground truth labels</li> <li>Can optionally perform side-by-side (sbs) comparison with a base model</li> <li>Supports fuzzy matching of predictions when fuzzy=True</li> </ul>"},{"location":"memory_rag/","title":"Memory RAG","text":"<p>Memory RAG is a simple approach that boosts LLM accuracy from ~50% to 90-95% compared to GPT-4. It creates contextual embeddings that capture meaning and relationships, allowing smaller models to achieve high accuracy without complex RAG setups or fine-tuning overhead.</p>"},{"location":"memory_rag/#quickstart","title":"Quickstart","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai):</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <p>To use Memory RAG, build an index by uploading documents and selecting a base open-source LLM. This is the model you'll use to query over your documents.</p> Python SDKREST API <p>Initialize the MemoryRAG client:</p> <pre><code>from lamini import MemoryRAG\n\nclient = MemoryRAG(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n</code></pre> <p>Define the PDF file to embed: <pre><code>lamini_wikipedia_page_pdf = \"https://huggingface.co/datasets/sudocoder/lamini-wikipedia-page/blob/main/Lamini-wikipedia-page.pdf\"\n</code></pre></p> <p>Embed and build the Memory RAG Index: <pre><code>response = client.memory_index(documents=[lamini_wikipedia_page_pdf])\n</code></pre></p> <pre><code>curl --location 'https://api.lamini.ai/alpha/memory-rag/train' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --form 'files=\"https://huggingface.co/datasets/sudocoder/lamini-wikipedia-page/blob/main/Lamini-wikipedia-page.pdf\"' \\\n    --form 'model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"'\n</code></pre> <p>Next, wait for training to complete by polling for status.</p> Python SDKREST API <pre><code>job_id = response['job_id']\n\nstatus = client.status(job_id)\n</code></pre> <pre><code>curl --location 'https://api.lamini.ai/alpha/memory-rag/status' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"job_id\": 1,\n    }'\n</code></pre> <p>Finally, query the model.</p> Python SDKREST API <p>Create a prompt: <pre><code>user_prompt = \"How is lamini related to llamas?\"\nprompt_template = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n {prompt} &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nprompt = prompt_template.format(prompt=user_prompt)\n</code></pre></p> <p>Pass the prompt to the Memory RAG model: <pre><code>response = client.query(prompt)\n</code></pre></p> <pre><code>curl --location 'https://api.lamini.ai/alpha/memory-rag/completions' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n How are you? &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\",\n        \"job_id\": 1\n    }'\n</code></pre>"},{"location":"self_managed/OIDC/","title":"User Authentication with OIDC","text":"<p>Note</p> <p>OIDC authentication is only available when self-managing Lamini Platform. Contact us to learn more.</p> <p>Lamini Platform supports Open ID Connect (OIDC) for user authentication. When enabled, only OIDC-authenticated users are able to access your Lamini instance. When an unauthenticated user tries to access your Lamini instance, they will be redirected to the OIDC identity provider you specify to log in. You can use any vendor-provided OIDC provider (like Auth0, Okta, AWS IAM, GCP Identity Platform, Azure Entra, and many more) or any internal service that adheres to the OIDC standard.</p> <p>After a user has signed in to Lamini Platform, they can create API keys and authenticate requests as described in API authentication.</p>"},{"location":"self_managed/OIDC/#setup-flow","title":"Setup flow","text":"<ol> <li>Determine the URI where your Lamini Platform instance will run.</li> <li>Create an application in your OIDC provider for Lamini and configure it.<ol> <li>Example configuration: Auth0<ol> <li>Application Type: <code>Regular Web Application</code></li> <li>Login URL: <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/login</code></li> <li>Callback URL <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/auth</code></li> <li>Logout URL <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> <li>Web origins <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> </ol> </li> <li>Example configuration: Google<ol> <li>Redirect URI: <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/auth</code></li> <li>Authorized Origin: <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> </ol> </li> </ol> </li> <li>Get the Application Client ID, Application Client Secret, and the OIDC Connect URL for your OIDC provider.<ol> <li>Example URL: Auth0: <code>https://&lt;YOUR-AUTH0-APP&gt;/.well-known/openid-configuration</code></li> <li>Example URL: Google: <code>https://accounts.google.com/.well-known/openid-configuration</code></li> </ol> </li> <li> <p>Configure OIDC in the <code>llama_config_edits.yaml</code> file for your Lamini install</p> <ol> <li>Set <code>disable_auth</code> to <code>False</code> to enable auth</li> </ol> <pre><code>auth:\n  disable_auth: False\n</code></pre> <ol> <li>Set <code>website</code> to the URI of your Lamini Platform instance</li> </ol> <pre><code>powerml:\n  website: \"https://&lt;LAMINI_INSTANCE_URI&gt;\"\n</code></pre> <ol> <li>Set the <code>client_id</code>, <code>client_secret</code>, and <code>server_metadata_url</code> values</li> </ol> <pre><code>auth_provider:\n  client_id: \"&lt;CLIENT_ID&gt;\"\n  client_secret: \"&lt;CLIENT_SECRET&gt;\"\n  server_metadata_url: \"&lt;OIDC_CONNECT_URL&gt;\"\n</code></pre> </li> </ol>"},{"location":"self_managed/aws_eks_setup/","title":"AWS EKS Setup","text":"<p>Note</p> <p>The Lamini installer is only available when self-managing Lamini Platform. Contact us to learn more.</p>"},{"location":"self_managed/aws_eks_setup/#summary","title":"Summary","text":"<p>This installation covers the steps of setting up an AWS EKS cluster, and creating a S3-backed NFS storage gateway for installing Lamini Platform.</p> <ul> <li>Prerequisites</li> <li>Install AWS CLI</li> <li>Configure AWS credential</li> <li>Create Key Pair</li> <li>Create EKS Cluster</li> <li>Set Up NFS</li> <li>Install Lamini Installer</li> </ul>"},{"location":"self_managed/aws_eks_setup/#prerequisites","title":"Prerequisites","text":"<p>An AWS account with GPU instances available, such as the G type. We recommend having sufficient resources with 8-GPU instance, such as the g6e.48xlarge.</p>"},{"location":"self_managed/aws_eks_setup/#install-aws-cli","title":"Install AWS CLI","text":"<p>Follow the AWS instruction to install the AWS CLI Check the AWS CLI installation.</p> <pre><code>aws --version\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#configure-aws-credential","title":"Configure AWS credential","text":"<p>Create and get the AWS Access Key ID and AWS Secret Access Key.</p> <p> </p> <p>Configure AWS credential.</p> <pre><code>aws configure\n</code></pre> <p></p> <pre><code>aws eks update-kubeconfig --name &lt;eks-cluster-name&gt;\n</code></pre> <p></p>"},{"location":"self_managed/aws_eks_setup/#create-key-pair","title":"Create Key Pair","text":"<p>Create Key Pair with AWS CLI, this is used to get SSH access to the nodes in the EKS cluster.</p> <pre><code>aws ec2 create-key-pair --key-name my-eks-keypair --query 'KeyMaterial' --output text &gt; my-eks-keypair.pem\nchmod 400 my-eks-keypair.pem\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#create-eks-cluster","title":"Create EKS Cluster","text":"<p>Use <code>eksctl</code> to manage EKS clusters. Eksctl is a \"battery-included\" tool, which manages many dependent aspects of EKS cluster. For example, it automatically install GPU plugins when requesting GPU instances, which is not the case when manually create with EKS web UI or <code>aws</code> CLI.</p> <p>To install eksctl on mac:</p> <pre><code>brew install eksctl\n</code></pre> <p>To Create a new EKS cluster:</p> <pre><code>eksctl create cluster \\\n  --name=&lt;name&gt; \\\n  --region=&lt;region&gt; \\\n  --node-type=&lt;type&gt; \\\n  --nodes 2 \\\n  --nodes-min=2 \\\n  --nodes-max=2 \\\n  --managed \\\n  --ssh-access \\\n  --ssh-public-key=my-eks-keypair\n</code></pre> <p>To add a new node group to an existing EKS cluster:</p> <pre><code>eksctl create nodegroup \\\n  --cluster=&lt;cluster-name&gt; \\\n  --name=&lt;node-type&gt;-group \\\n  --region=&lt;region&gt; \\\n  --nodes=1 \\\n  --nodes-min=1 \\\n  --nodes-max=1 \\\n  --node-type=&lt;node-type&gt; \\\n  --managed \\\n  --ssh-access \\\n  --ssh-public-key=my-eks-keypair\n</code></pre> <p>To remove a node group from an existing EKS cluster:</p> <pre><code>eksctl delete nodegroup \\\n  --cluster=&lt;cluster-name&gt; \\\n  --name=&lt;node-group-name&gt; \\\n  --region=&lt;region&gt;\n</code></pre> <p>To get access to a EKS cluster:</p> <pre><code>REGION=&lt;your-region&gt;\neksctl get cluster --region=${REGION}\n# This will write kube config for the cluster\neksctl utils write-kubeconfig --cluster=yaxiong-test-3 --region=${REGION}\nkubectl get node\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#set-up-nfs","title":"Set Up NFS","text":"<p>Create AWS S3 File Gateway. Type Storage Gateway in the search bar of AWS Console. </p> <p>Click on Create gateway. </p> <p>Enter gateway name and timezone. </p> <p>Launch EC2 instance as the gateway instance, choose the following options: Platform options: Amazon EC2; VPC network: Select the VPC of the created EKS cluster; Key pair: Select the created instance key pair; then click Launch the instance. </p> <p>Connect to AWS - select the IP address connection option and publicly accessible endpoint option. </p> <p>Activate gateway. </p> <p>Configure after activating the gateway. </p> <p>Create file share. </p> <p>Select the gateway that just created, set NFS protocol, and then create the S3 bucket.  </p> <p>Select the S3 that was just created.</p> <p></p> <p>Create the file share.</p> <p></p> <p>Add the file share client access restriction as needed.</p> <p> </p> <p>Note down the NFS IP and path that will be used in the NFS setup for Lamini installation. They are used as NFS_IP in NFS_SUBFOLDER_PATH for installing NFS provisioner when installing Lamini Platform.</p> <p></p>"},{"location":"self_managed/aws_eks_setup/#install-lamini-platform-with-installer","title":"Install Lamini Platform with installer","text":"<p>Follow the Kubernetes installation guide to install Lamini Platform with the installer.</p>"},{"location":"self_managed/disaster_recovery/","title":"Disaster recovery","text":""},{"location":"self_managed/disaster_recovery/#scenarios-and-actions","title":"Scenarios and actions","text":""},{"location":"self_managed/disaster_recovery/#accidentally-removeddeleted-a-pod-of-lamini-platform-from-k8s","title":"Accidentally removed/deleted a pod of Lamini Platform from K8s","text":"<p>No action needed. Kubernetes should restart the pod automatically.</p>"},{"location":"self_managed/disaster_recovery/#accidentally-removeddelete-a-secret-of-lamini-platform-from-k8s","title":"Accidentally removed/delete a secret of Lamini Platform from K8s","text":"<p>Need to recreate the secret. Otherwise redeploy or upgrade Lamini Platform will fail, as it misses the <code>imagePullSecret</code>.</p>"},{"location":"self_managed/disaster_recovery/#accidentally-delete-the-whole-namespace-of-lamini-platform-from-k8s","title":"Accidentally delete the whole namespace of Lamini Platform from K8s","text":"<p>Make sure you have pvc achived. Need to first reinstall <code>persistent-lamini</code>, restore the content of the newly-created pvc from the archived pvc, and then reinstall <code>lamini</code>.</p>"},{"location":"self_managed/kubernetes_install/","title":"Installing Lamini Platform on Kubernetes","text":"<p>Note</p> <p>The Lamini installer is only available when self-managing Lamini Platform. Contact us to learn more.</p> <p>Lamini Platform on Kubernetes enables multi-node, multi-GPU inference and training running on your own GPUs, in the environment of your choice.</p>"},{"location":"self_managed/kubernetes_install/#prerequisites","title":"Prerequisites","text":""},{"location":"self_managed/kubernetes_install/#tools","title":"Tools","text":"<p>You need to have a working Kubernetes cluster, and <code>python</code>, <code>helm</code>, <code>kubectl</code> installed.</p>"},{"location":"self_managed/kubernetes_install/#lamini-self-managed-license","title":"Lamini Self-Managed license","text":"<p>Contact us for access to the Kubernetes installer to host Lamini Platform on your own GPUs or in your cloud VPC.</p>"},{"location":"self_managed/kubernetes_install/#hardware-system-requirements","title":"Hardware system requirements","text":"<ul> <li>64 GB CPU memory</li> <li>1 TB disk</li> <li> <p>GPU memory: 2xHBM per GPU</p> </li> <li> <p>Example: AMD MI250 has 64GB of HBM, so Lamini requires 128GB of RAM per GPU.</p> </li> <li>Example: AMD MI300 has 192GB HBM, so Lamini requires 384GB of RAM per GPU.</li> </ul>"},{"location":"self_managed/kubernetes_install/#pvc","title":"PVC","text":"<p>Lamini requires a RWX PVC for storing all runtime data.</p> <p>You can use NFS and other storage solutions.    For example, you can set up a simple provisioner using <code>nfs-subdir-external-provisioner</code>:</p> <pre><code>helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner \\\n    --set nfs.server=&lt;NFS_IP&gt; \\\n    --set nfs.path=&lt;NFS_SUBFOLDER_PATH&gt;\n</code></pre> <p>Then proceed to create a PVC <code>lamini-volume</code> with <code>ReadWriteMany</code> access for installing Lamini Platform.</p>"},{"location":"self_managed/kubernetes_install/#gpu-operator","title":"GPU Operator","text":"<ul> <li> <p>For AMD:</p> <pre><code>git clone https://github.com/ROCm/k8s-device-plugin.git\nkubectl create -f k8s-device-plugin/k8s-ds-amdgpu-dp.yaml\n</code></pre> </li> <li> <p>For NVIDIA:</p> <pre><code>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \\\n  &amp;&amp; helm repo update\nhelm install --wait --generate-name \\\n  -n gpu-operator --create-namespace \\\n  nvidia/gpu-operator\n</code></pre> </li> </ul>"},{"location":"self_managed/kubernetes_install/#installation-steps","title":"Installation Steps","text":""},{"location":"self_managed/kubernetes_install/#obtain-the-installer","title":"Obtain the installer","text":"<p>Ask your Sales Engineer for the installer. The installer is a <code>.tar.gz</code> compressed file with all helm charts and scripts for installing Lamini Platform. You should work with your Sales Engineer for each installation or upgrade of Lamini Platform.</p> <p>You should keep any changes to the installer in a private repository for tracking purposes. Also ask your Sales Engineer to keep track of such changes.</p> <p>After obtaining the installer, extract it to a directory of your choice:</p> <pre><code># Make sure the installer name is used for directory name\n# so that you can track the version of the installer.\nINSTALLER_NAME=\"&lt;name&gt;\"\nmkdir -p ${INSTALLER_NAME}\ntar -xzf ${INSTALLER_NAME}.tar.gz -C ${INSTALLER_NAME}\n</code></pre> <p>The rest of the instructions are in the INSTALL.md file in the installer. You should operate under the directory of the installer.</p> <pre><code># Change to the installer directory\ncd ${INSTALLER_NAME}/lamini-kube-installer\n\n# Read the INSTALL.md file, open with your favorite editor\nvi INSTALL.md\n</code></pre>"},{"location":"self_managed/kubernetes_install/#update-helm_configyaml","title":"Update <code>helm_config.yaml</code>","text":"<ol> <li>Optional: If you already have <code>nfssubdir-external-provisioner</code> installed, set the <code>pvc_provisioner</code> to the <code>storageclass</code> name of defined by your installed <code>nfs-subdir-external-provisioner</code>.</li> </ol> helm_config.yaml<pre><code>pvc_provisioner: nfs-client\n</code></pre> <ol> <li>If you opt to use Lamini Platform provided NFS pvc provisioner, set the <code>pvcLamini.name</code> to the name of the PVC you want to use, and set <code>create</code> to <code>True</code>, and set <code>size</code> to the recommended <code>200Gi</code>, or work with your Sales Engineer to determine the size:</li> </ol> helm_config.yaml<pre><code>pvcLamini: {\n   name: lamini-volume,\n   size: 200Gi,\n   create: True\n}\n</code></pre> <p>if you have already created a PVC, set <code>name</code> to the name of the PVC, set <code>create</code> to <code>False</code>, you can    omit <code>size</code>:</p> helm_config.yaml<pre><code>pvcLamini: {\n   name: lamini-volume,\n   create: False\n}\n</code></pre> <ol> <li>Confirm the top-level platform <code>type</code> (one of: <code>amd</code>, <code>nvidia</code>, or <code>cpu</code>) matches your hardware.</li> </ol> helm_config.yaml<pre><code>type: \"amd\"\n</code></pre> <ol> <li>Update the distribution of inference pods.</li> </ol> helm_config.yaml<pre><code>inference: {\n   type: ClusterIP,\n   batch: 1,\n   streaming: 1,\n   embedding: 1,\n   catchall: 1\n}\n</code></pre> <p>The example above would create 4 pods using 4 GPUs in total. Each pod has 1 GPU. The example shows 1 inference pod allocated to <code>batch</code> inference, 1 pod dedicated only to <code>streaming</code> inference, 1 dedicated only to <code>embedding</code> inference (also used in classification), and 1 for the <code>catchall</code> pod, which is intended to handle requests for models that have not been preloaded on the <code>batch</code> pod. See Model Management for more details.</p> <ol> <li>Update the number of training pods and number of GPUs per pod:</li> </ol> helm_config.yaml<pre><code>training: {\n   type: ClusterIP,\n   num_pods: 1,\n   num_gpus_per_pod: 8\n}\n</code></pre> <p>We recommend minimizing the number of pods per node. For example, instead of 2 pods with 4 GPUs, it's better to create 1 pod with all 8 GPUs.</p> <ol> <li>Update the node affinity for the Lamini deployment. These are the nodes where Lamini pods will be deployed:</li> </ol> helm_config.yaml<pre><code>nodes: [\n   \"node0\"\n]\n</code></pre> <ol> <li>(Optional) If you want to use a custom ingress pathway, update the <code>ingress</code> field:</li> </ol> helm_config.yaml<pre><code>ingress: 'ingress/pathway'\n</code></pre>"},{"location":"self_managed/kubernetes_install/#generate-helm-charts-and-install-lamini-platform","title":"Generate Helm Charts and install Lamini Platform","text":"<p>Follow the INSTALL.md included in the installer for the detailed steps. The general steps are:</p> <ol> <li>Generate Helm charts with the provided shell script</li> <li>Install Lamini Platform with <code>helm install</code> or upgrade with <code>helm upgrade</code></li> </ol>"},{"location":"self_managed/model_management/","title":"Model Management","text":"<p>Note</p> <p>Editing the list of available models is allowed only when self-managing Lamini Platform. The list of models on Lamini On Demand is managed by Lamini. Contact us to learn more.</p> <p>Lamini Platform supports a variety of models. When self-managing Lamini, you control which models are preloaded, and how dynamic model loading will work.</p>"},{"location":"self_managed/model_management/#preloading-models","title":"Preloading models","text":"<p>To edit the list of preloaded models for your Lamini Self-Managed deployment, you need to modify the <code>llama_config.yaml</code> file:</p> <ol> <li> <p>Locate the <code>llama_config.yaml</code> file in your Lamini deployment's configuration directory.</p> </li> <li> <p>Look for the <code>batch_model_list</code> key under <code>multi_node_inference</code>. This list contains the models that are preloaded.</p> </li> <li> <p>Edit the <code>batch_model_list</code> to add or remove models as needed. Each model must be specified by its Hugging Face model identifier (e.g. <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> for Llama 3.1).</p> </li> </ol> <p>Be conservative with the number of models you preload - each model requires a significant amount of memory.</p>"},{"location":"self_managed/model_management/#dynamic-model-loading","title":"Dynamic model loading","text":"<p>Your inference GPU settings (defined in <code>helm_config.yaml</code> for Kubernetes installations) affect how dynamic model loading will perform.</p> <p>Inference requests for models that are not preloaded will be routed to <code>catchall</code> pods first. If a <code>catchall</code> pod is available, it will download the requested model from Hugging Face and load it into memory. If no <code>catchall</code> pods are available, requests will be routed to the other inference pods, which will download the requested model and load it into memory.</p> <p>Downloading and loading a model takes significant time. We recommend allowing 20-30 minutes for a model to become available after it's first requested. Loading a new model into memory can also mean that other models will be evicted from memory. This means that rapidly requesting many different models will result in poor performance.</p> <p>If you are experimenting with many different models, make sure to allocate enough <code>catchall</code> pods to handle the load without disrupting your other inference pods.</p> <p>We recommend focusing development on one model or a small set of models, and preloading them. We've seen the highest accuracy and performance gains come from improving data quality and tuning recipes, rather than testing many models hoping to find one that works significantly better out of the box.</p>"},{"location":"self_managed/model_management/#model-downloading","title":"Model downloading","text":"<p>You can use the following curl command to request Lamini Platform to download a model:</p> <pre><code>curl -X POST \"[YOUR_API_URL]/v1alpha/downloaded_models/\" \\\n    --header \"Authorization: Bearer [YOUR_API_KEY]\" \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\"hf_model_name\": \"[YOUR_MODEL]\"}'\n</code></pre> <p>You can also use the <code>Lamini</code> API to request downloading a model, see model_download.py for an example.</p>"},{"location":"tuning/dashboard/","title":"Dashboard","text":"<p>Your tuning dashboard makes it easy to see all your tuning jobs (finetuning, LoRAs, pretraining, etc.) at https://app.lamini.ai/tune.</p> <p></p> <p>You can easily access a playground for all your models and be able to share this view with your colleagues and friends. You can also view the logs to see what happened during tuning, and see the evaluation results of your model against its base model.</p> <p>Finally, you can also view the status of your tuning jobs.</p> <p></p>"},{"location":"tuning/evaluation/","title":"Evaluation UI","text":"<p>Evaluation is key to understanding the performance of your LLM. We provide a few different ways to evaluate your LLM, depending on your use case.</p> <p>You can access our dashboard to see the evaluation results of your model against its base model.</p> <p></p>"},{"location":"tuning/hyperparameters/","title":"Hyperparameters","text":"<p>Lamini tuning supports most hyperparameters in HuggingFace's training arguments, as well as some Lamini-specific options.</p> <p>These can be set in the <code>tune</code> method:</p> <pre><code># code/hyperparameters.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\ndata = [\n    {\n        \"input\": \"What is Lamini? Is it like a robot or a computer program?\",\n        \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\",\n    }\n]\n\nresults = llm.tune(data_or_dataset_id=data, finetune_args={\"learning_rate\": 1.0e-4})\n</code></pre> <p>See Memory Tuning for use-case specific suggestions.</p>"},{"location":"tuning/hyperparameters/#finetune_args","title":"finetune_args","text":"<ul> <li>max_finetuning_examples (int, optional)</li> <li>Default: size of the dataset</li> <li> <p>Sets the maximum number of data points for fine-tuning. If not set, the model is fine-tuned on the entire dataset.</p> </li> <li> <p>max_steps (int, optional)</p> </li> <li>Default: <code>100</code></li> <li>Specifies the total number of training steps to perform.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>gradient_accumulation_steps (int, optional)</p> </li> <li>Default: <code>2</code></li> <li>Number of update steps to accumulate the gradients for, before performing a backward/update pass.</li> <li>Usage note: a higher setting can improve memory efficiency and thus reduce training time, often with a neutral effect on model accuracy.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>learning_rate (float, optional)</p> </li> <li>Default: <code>9.0e-4</code></li> <li>The initial learning rate for the fine-tuning.</li> <li>This parameter is passed to HuggingFace's Transformers TrainingArguments.</li> <li> <p>Usage note: see the Memory Tuning section for tips on setting learning rate.</p> </li> <li> <p>save_steps (int or float, optional)</p> </li> <li>Default: <code>60</code></li> <li>Number of update steps between two checkpoint saves.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>max_length (int, optional)</p> </li> <li>Default: <code>2048</code></li> <li>Specifies the maximum sequence length for the forward pass, acting as the block size for the model.</li> <li>Should be a power of 2, no larger than <code>8192</code>.</li> <li> <p>Usage note: <code>max_length</code> should be at least as large as the size of your datapoints. If training with large datapoints is not converging, increasing this value may help. However, larger values of <code>max_length</code> increase training time, and very large values will exhaust GPU memory. There's often room to reduce the size of your datapoints so a smaller <code>max_length</code> can be used.</p> </li> <li> <p>optim (str, optional)</p> </li> <li>Default: <code>\"adafactor\"</code></li> <li>The optimizer to use: <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_apex_fused</code>, <code>adamw_anyprecision</code> or <code>adafactor</code>.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>r_value (int, optional)</p> </li> <li>Default: <code>64</code></li> <li> <p>Specifies the size of the LoRA (Low-Rank Adaptation) component.</p> </li> <li> <p>index_method (str, optional)</p> </li> <li>Default: <code>\"IndexIVFPQ\"</code></li> <li> <p>The index method used for approximate nearest neighbor search of high-dimensional vectors: <code>IndexIVFPQ</code>, <code>IndexHNSWPQ</code>,<code>IndexHNSWFlat</code>, <code>IndexFlatL2</code>, <code>IndexPQ</code></p> </li> <li> <p>index_k (int, optional)</p> </li> <li>Default: <code>2</code></li> <li> <p>Determines the number of nearest neighbors to consider.</p> </li> <li> <p>index_max_size (int, optional)</p> </li> <li>Default: <code>65536</code></li> <li> <p>Maximum size of the index.</p> </li> <li> <p>index_pq_m (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Number of factors of product quantization.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code> or <code>IndexPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_pq_nbits (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Number of bits in which each low-dimensional vector is stored. Range: [1, 16]</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code> or <code>IndexPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_ivf_nlist (int, optional)</p> </li> <li>Default: <code>2048</code></li> <li>Number of buckets during clustering for IVFLAT.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_ivf_nprobe (int, optional)</p> </li> <li>Default: <code>48</code></li> <li>Number of buckets to search during the first step of IVFLAT.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_m (int, optional)</p> </li> <li>Default: <code>32</code></li> <li>Range: [4, 64]. Used in HNSW (Hierarchical Navigable Small World Graph) algorithm.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_efConstruction (int, optional)</p> </li> <li>Default: <code>16</code></li> <li>Expansion factor at construction time for HNSW. Range: [8, 512]</li> <li> <p>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_efSearch (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Expansion factor at search time for HNSW.</li> <li>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</li> </ul>"},{"location":"tuning/hyperparameters/#immutables","title":"Immutables","text":"<p>The following configs are only supported at their default values:</p> <ul> <li>batch_size: 1</li> <li>early_stopping: false</li> <li>num_train_epochs: Will be overriden by <code>max_steps</code></li> <li>temperature: 0</li> <li>lora_alpha: 1</li> </ul>"},{"location":"tuning/hyperparameters/#gpu_config","title":"gpu_config","text":"<ul> <li>gpus: (int, optional)</li> <li>Default: <code>1</code></li> <li>Number of GPUs per node to use for the tuning job.</li> <li>nodes: (int, optional)</li> <li>Default: <code>1</code></li> <li>Number of nodes (machines containing multiple GPUs) to use for the tuning job.</li> </ul> <pre><code>gpu_config = {\n    \"gpus\": 4,\n    \"nodes\": 1,\n}\n</code></pre> <p>The Lamini On-Demand allows a maximum of GPUs and nodes based on our server availability. If you are on Lamini Reserved or Self-managed, you can specify any number of GPUs and nodes within your provisioned cluster size. Your job will be queued until the requested number of GPUs and nodes is available.</p> <p>If the required GPUs and nodes are not available, the configuration defaults to the system limit, and the job is queued until the resources become available. When using multiple nodes, specify the number of GPUs required per node.</p> <p>Examples:</p> <pre><code>gpu_config = {\"gpus\": 8, \"nodes\": 1}  # total 8 GPUs\ngpu_config = {\"gpus\": 8, \"nodes\": 2}  # total 16 GPUs\ngpu_config = {\"gpus\": 4, \"nodes\": 2}  # total 8 GPUs\ngpu_config = {\"gpus\": 9, \"nodes\": 1}  # error, assuming max GPUs per node is 8\n</code></pre>"},{"location":"tuning/hyperparameters/#data_or_dataset_id","title":"data_or_dataset_id","text":"<ul> <li>data_or_dataset_id (JSONL or CSV file or dataset ID of an already uploaded dataset, required)</li> <li>Default: no default</li> <li>Specifies the dataset to use for the tuning job.</li> </ul>"},{"location":"tuning/large_data_files/","title":"Large Data Files","text":"<p>If you are tuning on a large file of data, you can use the <code>upload_file</code> function to first upload the file onto the servers.</p> <p>Here is an example with a <code>test.csv</code> file:</p> <pre><code>// code/test.csv\n\nuser,answer\n\"Explain the process of photosynthesis\",\"Photosynthesis is the process by which plants and some other organisms convert light energy into chemical energy. It is critical for the existence of the vast majority of life on Earth. It is the way in which virtually all energy in the biosphere becomes available to living things.\n\"What is the capital of USA?\", \"Washington, D.C.\"\n</code></pre> <p>You can use the Lamini to tune on this file directly by uploading the file and specifying the input and output keys.</p> <pre><code># code/large_data_files_csv.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\ndataset_id = llm.upload_file(\"test.csv\", input_key=\"user\", output_key=\"answer\")\n\nllm.tune(data_or_dataset_id=dataset_id)\n</code></pre> <p>Alternatively, you can also use <code>jsonlines</code> files</p> Using <code>test.jsonl</code> <pre><code>// code/test.jsonl\n\n{\"user\": \"Explain the process of photosynthesis\", \"answer\": \"Photosynthesis is the process by which plants and some other organisms convert light energy into chemical energy. It is critical for the existence of the vast majority of life on Earth. It is the way in which virtually all energy in the biosphere becomes available to living things.\"}\n{\"user\": \"What is the capital of USA?\", \"answer\": \"Washington, D.C.\"}\n</code></pre>      Then tune on this file using the `tune` function.      <pre><code># code/large_data_files_jsonl.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\ndataset_id = llm.upload_file(\"test.jsonl\", input_key=\"user\", output_key=\"answer\")\n\nllm.tune(data_or_dataset_id=dataset_id)\n</code></pre>"},{"location":"tuning/memory_tuning/","title":"Memory Tuning","text":"<p>Memory Tuning is a research capability from Lamini that transforms how LLMs learn and recall information, with precise memory. It enables:</p> <ol> <li>Extreme Accuracy (95%+): Eliminate hallucinations by injecting precise facts directly into the model's memory, removing accuracy ceilings on many tasks</li> <li>Efficient Scaling: Start with just 10 examples and scale to 100,000+ facts, handling both far fewer and more examples than fine-tuning</li> <li>Cost-Effective Mini-Agents: Use smaller, memory-tuned models instead of large foundation models, while maintaining high accuracy</li> <li>Universal Compatibility: Works with any open source LLM through a single, simple API</li> <li>Low Latency: Achieve fast inference times by leveraging efficient memory access patterns</li> </ol> <p>Memory Tuning works by embedding precise, factual data inside the LLM's memory, through millions of adapters in a mixture of expert adapters. This transforms any open foundation model into a Mixture of Memory Experts (MoME, pronounced \"mommy\") that can recall facts with photographic accuracy, by selectively routing across a team of specialized experts. The result is a model that maintains its general reasoning capabilities, while having near-perfect recall of your specific data \u2014 to 95% or 99%+ accuracy on tasks that routinely get as low as 0% or 50% on state-of-the-art models like GPT-4 + RAG.</p> <p>Memory-tuned models can perform factual reasoning: Memory Tuning allows your LLMs to keep their general reasoning capabilities, while committing specific factual data to their weights as memory.</p>"},{"location":"tuning/memory_tuning/#notebook-example","title":"Notebook example","text":"<p>Check out our notebook example that answers questions about a Python class!</p> <p>We've also partnered with Meta to create a notebook that shows how to use Memory Tuning to improve a text-to-SQL model from 30% to 95% accuracy.</p> <p>Working through the notebook will give you a good sense of how to use Memory Tuning, and you can do it all within the Lamini On-Demand plan.</p>"},{"location":"tuning/memory_tuning/#principles-for-memory-tuning","title":"Principles for Memory Tuning","text":"<p>Talk to our team: We're happy to help you get started with the best recipe for your use case.</p> <p>Memory Tuning is a research capability. We've found that the following best practices help:</p> <p>Andrej Karpathy's A Recipe for Training Neural Networks is a great summary of the phased, iterative approach you should take to Memory Tuning (even though many of the specific examples in that article don't apply to Memory Tuning).</p> <ol> <li> <p>Become one with the data</p> <ul> <li>Deeply understand your dataset and your eval, and refine them to high quality</li> </ul> </li> <li> <p>Set up the end-to-end training/evaluation skeleton</p> <p>Before you start Memory Tuning, measure the baseline accuracy on:</p> <ol> <li>the base model</li> <li>base model + prompt tuning</li> <li>base model + prompt tuning + RAG</li> </ol> </li> <li> <p>Overfit</p> <ul> <li>Find a Memory Tuning recipe that's accurate on your facts, even just for one example, before scaling up your data</li> </ul> </li> <li> <p>Regularize</p> <ul> <li>Scale up your data and check generalization performance</li> </ul> </li> <li> <p>Optimize</p> <ul> <li>Continue iterating now that you have a solid foundation</li> </ul> </li> </ol> <p>Don't skip any of these steps!</p>"},{"location":"tuning/memory_tuning/#example-memory-tuning-settings","title":"Example Memory Tuning settings","text":"<p>Tuning hyperparameters can be a bit of an art. Where should you start experimenting?</p> <ul> <li><code>learning rate</code></li> <li><code>max_finetuning_examples</code></li> <li><code>max_steps</code></li> <li><code>gradient_accumulation_steps</code></li> </ul> <p>See Hyperparameters for the complete list of options.</p>"},{"location":"tuning/memory_tuning/#when-experimenting-with-a-small-dataset-100-facts","title":"When experimenting with a small dataset (&lt;100 facts)","text":"<pre><code>llm.train(data_or_dataset_id=data, finetune_args={\"max_steps\": 50, \"r_value\": 32, \"learning_rate\": 0.0003})\n</code></pre> <ul> <li>We recommend increasing <code>max_steps</code> when working with a larger dataset.</li> </ul>"},{"location":"tuning/memory_tuning/#factual-qa-from-pdfs-20-pdfs-800-facts","title":"Factual Q/A from PDFs (20 PDFs, 800+ facts)","text":"<pre><code>{\n  \"max_steps\": 500,\n  \"learning_rate\": 0.00009\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#text-to-sql-100-queries","title":"Text-to-SQL (100 queries)","text":"<pre><code>{\n  \"max_steps\": 500,\n  \"learning_rate\": 0.00001\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#factual-qa-on-10000-facts","title":"Factual Q/A on 10,000 facts","text":"<pre><code>{\n  \"gradient_accumulation_steps\": 4,\n  \"index_k\": 2,\n  \"index_max_size\": 65536,\n  \"index_method\": \"IndexFlatL2\",\n  \"learning_rate\": 0.0003,\n  \"max_length\": 512,\n  \"max_steps\": 10000,\n  \"index_ivf_nlist\": 2048,\n  \"max_finetuning_examples\": 10000,\n  \"r_value\": 64\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#experiment-with-learning-rate","title":"Experiment with learning rate","text":"<p>For training jobs with less than 300 steps, a grid search approach can be effective. You can run multiple jobs on a subset of the data with a range of learning_rates to find which learning rate has a better loss curve. Once that is found, you can expand the training to the larger dataset with this best learning_rate.</p> <pre><code>from lamini import Lamini\n\nlamini.api_key = \"&lt;key&gt;\"\n\ndef main():\n    llm = Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\n    dataset = your_dataset_goes_here\n\n    try:\n        start = time.time()\n        dataset_id = llm.upload_data(dataset)\n        end = time.time()\n        print(f\"Uploaded dataset in {end - start} seconds\")\n    except Exception as e:\n        print(f\"Failed to upload dataset: {e}\")\n        return\n\n    learning_rates = [0.0009, 0.0003, 0.00009,  0.00003, 0.000003, 0.000009]\n\n    for lr in learning_rates:\n        print(f\"Training with lr={lr}\")\n\n        try:\n            results = llm.train(\n                dataset_id,\n                use_cached_model=False,\n                finetune_args={\n                    \"learning_rate\": lr,\n                    \"max_steps\":300,\n                },\n                gpu_config={\n                    \"gpus\": 2,\n                    \"nodes\": 1,\n                }\n            )\n            print(f\"Training results: {results}\")\n        except Exception as e:\n            print(f\"Failed to train model: {e}\")\n            continue\n\ndef load_training_data():\n    &lt;\u2014\u2014code to gather data\u2014\u2014&gt;\n</code></pre>"},{"location":"tuning/memory_tuning/#specifying-gpus-and-nodes","title":"Specifying GPUs and nodes","text":"<p>Specifying additional GPUs and/or nodes can significantly reduce model tuning time, which is especially beneficial when working with large datasets.</p> <p><code>llm.train</code> takes an optional <code>gpu_config</code> argument that lets you specify the number of GPUs and nodes to use for tuning. See Hyperparameters for more details.</p> <p>If you are self-managing Lamini Platform, you can specify any number of GPUs and nodes within the cluster size you've provisioned.</p> <p>Your job will be queued until the requested number of nodes and GPUs are available.</p>"},{"location":"tuning/memory_tuning/#learn-more","title":"Learn more","text":"<ul> <li>See how a Fortune 500 company used Memory Tuning in our case study</li> <li>Read more in our blog post</li> </ul>"},{"location":"tuning/memory_tuning/#known-issue-tuning-on-a-previously-tuned-model","title":"Known issue: Tuning on a previously tuned model","text":"<p>Submitting a tuning job on a model is not currently supported. We are evaluating the feasibility of supporting continued tuning on previously tuned models. Feel free to contact us</p>"},{"location":"tuning/memory_tuning/#workaround","title":"Workaround","text":"<p>To include additional data, submit a new tuning job with the new data on the base model instead of adding the data to a previously tuned model. If your use case requires more than 500 data points, reach out to us for support. with any questions or concerns.</p>"}]}